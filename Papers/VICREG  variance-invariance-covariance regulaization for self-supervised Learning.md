# Questions
### Do we need both network for inference / Which network do we chose ? 
### What is collapse ? Are there different way of collapsing ? 
- All embedding vectors collapse to a trivbial constant solution 
- Dimentional collapse: the embedding vectors end up spanning a lower-dimensional subspace instead of the entire available embedding space. See [UNDERSTANDINGDIMENSIONALCOLLAPSE INCON-TRASTIVESELF-SUPERVISEDLEARNING](https://arxiv.org/pdf/2110.09348.pdf)
- 
### Why do we need both Covariance and Variance loss terms ? 
### What is the benefit of having different archictectures and different weights ? 

### Why decorelating at embeddings lvl also decorrelate at representation lvl ? 

# Minor questions
### What is LARS optimizer ? 

