{"data":{"allMdx":{"nodes":[{"fields":{"slug":"/Engineering/Distributed Hash Table/","title":"Distributed Hash Table"},"frontmatter":{"draft":false},"rawBody":""},{"fields":{"slug":"/Engineering/Feature Flags/","title":"Feature Flags"},"frontmatter":{"draft":false},"rawBody":""},{"fields":{"slug":"/Engineering/Prometheus/","title":"Prometheus"},"frontmatter":{"draft":false},"rawBody":""},{"fields":{"slug":"/ML/Gradient Descend/","title":"Gradient Descend"},"frontmatter":{"draft":false},"rawBody":""},{"fields":{"slug":"/Programming/Quicksort/","title":"Quicksort"},"frontmatter":{"draft":false},"rawBody":""},{"fields":{"slug":"/Courses/Coursera/Learning how to learn/Chunking/","title":"Chunking"},"frontmatter":{"draft":false},"rawBody":""},{"fields":{"slug":"/Courses/Coursera/Learning how to learn/Illusion of Competence/","title":"Illusion of Competence"},"frontmatter":{"draft":false},"rawBody":""},{"fields":{"slug":"/Courses/Coursera/Learning how to learn/Imposter syndrome/","title":"Imposter Syndrome"},"frontmatter":{"draft":false},"rawBody":""},{"fields":{"slug":"/Courses/Coursera/Learning how to learn/Overlearning/","title":"Overlearning"},"frontmatter":{"draft":false},"rawBody":""},{"fields":{"slug":"/Courses/Coursera/Learning how to learn/Procrastination/","title":"Procrastination"},"frontmatter":{"draft":false},"rawBody":""},{"fields":{"slug":"/Courses/Coursera/Learning how to learn/Tasks lists/","title":"Tasks Lists"},"frontmatter":{"draft":false},"rawBody":""},{"fields":{"slug":"/Coursera - Learning How to Learn/","title":"Week 1"},"frontmatter":{"draft":false},"rawBody":"# Week 1\n### 2 States of the mind: Focused and Diffused. \n- The focused mode is when we are focusing on a problem, intensely using pre-built thought pathways in our minds. \n- The diffused mode is when we are not focusing (doing sport, sleeping, cooking, ...). This disengagement of the mind from a particular subject allows higher level abstraction to be built\n\n### The are 2 (main) state of memory: working and Long term memories\n- Working memory is used for short term tasks. We have up to 4 slots or `chunks` of information we can store in our working memory. These slots are temporary so we will forget them pretty quickly.\n- Long term memory is used for keeping information for a long period of time. It is not easy to add new memories there.\n\nTo build knowledge is to create `stable` pathways in our minds. Moving thought processes and memories from short term to long term srtorage. \n\n\n### Learning habits\nTo learn something properly, we need to:\n1. work hard on it. Making the effort to experience new pathways. Being passive is way less productive than active in the learning process (for example watchiung a video <<<< doing the implementation yourself). \n2. Then relax, sleep, meditate, cook, do something else. This freeze the attention and allows your brain to assimilate the information. During this step the brain will digesty the information and build new neurological pathways. Sleeping is very important to build this pathways.\n3. Repeat 1. and 2. ! This process helps to migrate the memory from working to long term storage.\n\n### The Pomodoro technique\nIf you are having a hard time starting a new study, because it is hard, especially if you procrastinate, the pomodoro techniques helps you: \n1. Use a timer, give yourself 25 min (you can adapt) of study the material\n2. Take a break. This gives you a (small) reward after the effort. Which motivates you further and reduces the pain / angst again this study\n\n## Take away\n- Efficient learning involves going back and forth between Focused and Diffused modes.\n- Metaphors provide powerful techniques for learning \n- Learning something difficult takes time. Acknowledge it ! \n- Practice makes knowledge permanent\n- You have to add rest in between focus events (use pomodoro if need be)\n- Using `Spaced repetition` is very good to migrate information from Working to Long term memory. The Anki app is one tool do follow Spaced repetition.\n- Sleeping is very important to learn:\n\t- Clean brains \n\t- Is healthy \n\t- Clears unused pathways and strengthen the others ! \n\nStudying something just before going to bed is also powerful. Your brain will work harder on it over night.\n\n__________________\n__________________\n\n\n# Week 2\n\n## Chunking\nA chunk is a compact packages of information that your mind can easily access.  \nChunking is a logical package of pieces of information to make them easier to work with this concept.\n\nThe human can sustain 4 chunks in his working memory.\nA chunk is a network of neurones that are used to firing together.  \n\n### How to form a chunk ? \n1. You need to be in focus mode. no TV or anything else. Chunking relies on building new patterns using already existing one. This does not happen well if we are not focused.\n2. You need to understand the concept you are looking to chunk. This can be done by alternating btw Focused and Diffused mode. \n\t**But understanding how a problem is solved does not mean you create a chunk**. You need to **do it**, and make sure you can use this knowledge.\n3. It is important to build **context**. which is about surrounding information connecting them to the chunk. For example doing problems related to the original chunk. This will create new neural connections to the chunk you are building which makes it easier to come back to in different situations.\n\nLearning is a 2 step process.. Chunking is a `bottom-up` way to build a chunk (one by one, to learn a specific thing). Practice will increase the number of chunks related to each other. Using the big-picture (diffused mode) you will be able to connect chunks together. This builds the **Context** that relate chunks into one another.\n![[Pasted image 20220124212910.png|600]]\n\n![[Pasted image 20220124212611.png|500]]\n\n\n## Illusions of competence in Learning\n### Importance of Recall\nIt is better to recall the information of a text just read rather than re-reading the material. The learning is faster and much deeper.\n**The retrieval process itself is useful for remembering.**\n\n### Illusion of competence\nReading the solution of a solution, and thinking `I understand what they did here`. This is not knowing and you will not improve.\n\nIn the same way, reading google solutions will keep the illusion of competence.\n\nre-reading and highlirting too much text promotes ILLUSION OF Competence. \n\n--> **Testing yourself promotes long term learning via stabilising activations patterns in a large brain area.**\n--> Do Mini testing !!!!!!!!\n--> You can train to recall your information in different rooms in order to dissociate the knowledge from where you learned it.\n\n### Value of making mistakes\nMistakes are useful during mini testing. These mistakes help correct your thinking.\n\n\n\n## Seeing the bigger Picture\n### Library of chunks\n\nHaving many chunks helps to have a library of knowledge. You need to practice with growing chunks in order to remember and srtill be able to recall / use them.\n\n### Two ways to solve problem:\n1. Sequential thinking, using the focused mode\n2. or Holistic (global) approahc. Using intuition / Diffuse modes. A solutuon the diffuse mode provide should be verified in a focused mode.\n\n\n## Interleaving\n- **Overlearning** is when you work too much on the same material within the **same session** . It is still usefull to master something, but be careful when to use it.\n- Doing also the same thing you already know in further sessions is not good and give an illusion of competence again. \n- We need to do a **Deliberate practice** which is about balancing your study, studying what you find difficult.\n- Einstellung: an idea you already have in mind prevent a better idea or solution of being found.  You need to make sure that your original intuition is not false.\n- **Interleaving**: Once you have learned a new material / chunk and are not very familiar with it, interleaving is about balancing practicing with problems of different types around this chunk.\n\nInterleave is very important. it helps to build this creative pattern that you can rely on in new problems.\n\n\n\n## Summary\n- Chunks are pieces of information that are bound together through use and through meaning.\n- Chunk is a single easy item to access that uses one slot in the working memory. \n- Chunks are build with Focused attention, understanding and practice\n- Simple Recall (remembering the key points without looking at the page) is a powerful way to build chunks. \n\t- Bonus to recalling memory in different locations.\n- `Transfer` is the fact that you can relate chunks over different domains. They will help you learn new chunks\n- Interleaving learning by practicing your choice of concepts and techniques. This helps at becoming flexible with which chunks to use\n- Test yourself !!! this is a good way of avoiding the illusions of competence.\n- Mistakes are good.\n- Avoid practicing at easy stuff.\n- Einstellung is when an idea you already have in mind prevents a better idea / solutions from being found.\n- L:aw of Serendipity: just learn one thing, and you will see that the next one will be a little bit simpler to learn.\n"},{"fields":{"slug":"/","title":"Brain Dam"},"frontmatter":{"draft":false},"rawBody":"# Brain Dam\n\nThis website contains notes following (or trying to) the [Zettelkasten principles](https://zettelkasten.de/introduction/).\n\nI will be keeping note of many kinds for my work (ML, engineering, papers, ...) and personal knowlegde (finances) as well as interview preperations and many others !\n\n\n# How do I maintain these notes ? \nI used [Obsidian](https://obsidian.md/) to take the notes. The vault points directly to the root of the [git repo](https://github.com/thomassajot/brain-dam) that host this website.\n\nObsidian natively supports math (latext) and graphs (mermaid) in Markdown. Plugins are manually added to the git repo in order to support these as well. \n\n\n"},{"fields":{"slug":"/Azure/Azure pipeline/","title":"Azure Pipeline"},"frontmatter":{"draft":false},"rawBody":"### Example azure pipeline with conda / mamba: \n[Fermitools-conda/master/azure-pipelines.yml](https://github.com/fermi-lat/Fermitools-conda/blob/master/azure-pipelines.yml) \n(or a fixed point in history)\n[Fermitools-conda/@commit/azure-pipelines.yml](https://github.com/fermi-lat/Fermitools-conda/blob/8c9676019c917be688ace1a1b8ddc5b24471bf92/azure-pipelines.yml)\n\n\n### In pipeline.yml using `bash` vs `script` commands\nSee documentation [consider-bash-or-pwsh](https://docs.microsoft.com/en-us/azure/devops/pipelines/scripts/cross-platform-scripting?view=azure-devops&tabs=yaml#consider-bash-or-pwsh)"},{"fields":{"slug":"/Azure/Azure-ml/","title":"Documentation"},"frontmatter":{"draft":false},"rawBody":"# Documentation\n- [azureml-cheatsheets](https://azure.github.io/azureml-cheatsheets/)\n\n"},{"fields":{"slug":"/Engineering/API Design/","title":"API Design"},"frontmatter":{"draft":false},"rawBody":"Pagination\nCRUD Operations."},{"fields":{"slug":"/Engineering/Availability/","title":"SLA / SLO"},"frontmatter":{"draft":false},"rawBody":"What happens if the system fails. How fault tolerant is the system. The percentage of time where the system is satisfying its primary functions.\n# SLA / SLO\nSystem level agreement. System level objective. SLO are components of SLA. \n\n# Nines\nThe percentage of time where the system is running according to SLA is usually expressed in percentages with `9`, like 99% availability in a year, or 99.9%, 99.999%, ... Which are 2, 3, and 5 nines.\n\nSystems with `Highly Availability (HA)` are systems with more than 5 nines ~5 min of downtime. It is a official term \n# Redundancy\nRemoving Single Points of Failure is a way to improve availability. Redundancy is a way to duplicates (or more) your servers.   \n`Passive redundancy`: If a machine dies, the other machines already there will pick up the traffic.  \n`Active redundancy`: Only one or few of the machines doing work. If this one fail, the other machines are going to know and pick up the work. For example a Leader election, if the leader machine fails another machine will become leader."},{"fields":{"slug":"/Engineering/Caching/","title":"Cache for read"},"frontmatter":{"draft":false},"rawBody":"\n# Cache for read\n\n# Update Cache on Writes\n## Write through Cache\nHave the cache and DB in sync when there is a write. Downside is that it takes time.\n\n## Write back Cache\nThe server updates only the cache upon a write, then goes back to client. This makes the cache out of sync with the Database. The system will asynchronously update the DB. At every intervals, or when the cache is filled up (when you need to `evict` the cache).  \n\nDownside: if something happens to the cache, before there is a write to DB, then you may loose data.\n\n\n# What happens if we have many servers that do caches ?\nThe first client writes a comment, it is cached to Server 1 and saved in DB. Client 2 reads the comment from another server 2, this server 2 caches the comments.  \nThe first clients updates the comment. How do we make sure the client 2 gets the new comments that is still cached in server 2 as the old text ? Caches can become `stale` if they have not been updated properly.\n\nFor example, a solution would be to have a single cache (like [[Redis]]) out of the servers, that would be the single source of truth. \n\nFor some features, we may not care about `staleness`, for example view count on youtube videos.\n\n```mermaid \ngraph LR\nClient_1\nClient_2\nServer_1\nServer_2\nRedis\nDatabase\n\nClient_1 --> Server_1\nClient_2 --> Server_2\nServer_1 --> Redis\nServer_2 --> Redis\n\nRedis --- Database\n```\n\n# When to use cache\nIf you deal with data that is immutable, caching is easy and great. If the data is mutable, that is a big work. \n- using Cache is you use immutable data. \n- If you have a single thing that read / write data then we can use cache\n- If you don't care about consistency / staleness of data, you can use caching\n- If you are able to design your system to get rid of stale data (in a distributed manner if your system uses it) then you can use cache\n\n# Cache eviction\nYou cannot store infinite data, or you have stale data, you need to get rid of data in Caches (`eviction`).   \nUsing schedules / policies:\n- LRU: Get rid of the Least Recently Used data in cache. \n- Least frequently used: Remove the least frequently used\n- LIFO or FIFO\n- Randomly\n- Every day"},{"fields":{"slug":"/Engineering/Client-Server Model/","title":"Client-Server Model"},"frontmatter":{"draft":false},"rawBody":"```mermaid\ngraph TD\nClient\nServer\nDNS\n\nClient -- Step 1 --> DNS\nClient --  Step 3 <br> HTTP request --> Server\nDNS -- \"Step 2 - IP@\" --> Client\nServer -- Step 4 <br> HTTP response --> Client\n```\n\nServer are waiting for requests from clients. Server licenses to requests on Ports. There are 16000 ports to listen to.\n- HTTP uses port 80\n- HTTPS uses port 443\n- "},{"fields":{"slug":"/Engineering/Configuration/","title":"Configuration"},"frontmatter":{"draft":false},"rawBody":"Configuration is a set of parameters / constants that your application uses. These parameters are configurable in a file in order to be edited easily.  \nYou can use it for [[Feature Flags]].\n\nThere are 2 types of configurations:\n- `Static` : You have to change the configuration file, then deploy the application. Pro's is that if the configuration change breaks the app, testing might pick it up. Static configuration is usually a bit safer but it takes longer to see the impact due to deployment.\n- `Dynamic` is done on the live system. It is more complex, because it is backed by a DB that the system is querying in order to pick up the update. This is beneficial as it allows the build of a UI in order to change the system live. Pro's you can make changes and see their changes fast. Con's you do not have the same testing framework.\n\nUsually we use dynamic configuration and build added guarantees to prevent too many issues when changes configurations, for example:\n- not everyone can do the configuration change, ie access control\n- add a review process\n- apply the update every hour for example\n- deploy only to a few users\n\n\n"},{"fields":{"slug":"/Engineering/Consistent hashing/","title":"Consistent hashing with bounded load"},"frontmatter":{"draft":false},"rawBody":"[High level explanation](https://www.toptal.com/big-data/consistent-hashing)\n[Tutorial with more details](https://docs.openstack.org/swift/latest/ring_background.html)\n\nGeneral Idea:\n1. Hash all the servers IDs (UUID, IP address, ...) and \"map\" them onto the unit circle.\n1. Hash the object you want to save (like username). Then \"map\" this hash onto the unit circle\n1. The location where to store an object is on the closest server on the unit circle in the anti-clockwise direction (for example). \n\nIf a node crashes, we need only to move this nodes data, ie `K/N` values, `K` being the number of keys and `N` the number of nodes / servers in the pool.  \nExample of 3 servers 5 keys to be mapped.\n![[Pasted image 20211110210818.png|700]]\n\nIt is better to have multiple hashes for a given servers (10x, 50x, 1000x ?) so that selecting the closest server for an object is more uniform. The number of keys per servers (10x, ...) depends on the server capacity. This is called the servers `weight`. \nExample of 3 servers with 10 keys per server.\n![[Pasted image 20211110211244.png|700]]\n\n# Consistent hashing with bounded load\n\n# Questions \n- How do you support duplication over multiple server to be resilient if a machine goes down ? Maybe we can assign a key to the 2 closest servers in the circle ? z\n\n\n# Other strategy\nSee [[Rendez-vous Hashing]]  \nSee [[Jump hashing]]"},{"fields":{"slug":"/Engineering/Design Documents/","title":"Design Documents"},"frontmatter":{"draft":false},"rawBody":"Advices about writing design Docs:\nhttps://www.industrialempathy.com/posts/design-docs-at-google/\nhttps://www.industrialempathy.com/posts/design-doc-a-design-doc/"},{"fields":{"slug":"/Engineering/Etcd/","title":"Etcd"},"frontmatter":{"draft":false},"rawBody":"This is a [[Key-Value Stores|Key-Value Store]] that is [[Availability|highly available]] and Strongly Consistent store.  \nIt implements the RAFT consensus algorithm.\n\nWe can use this DB to implement leader election for our servers. You have multiple servers communicating with ETCD, and at any given point in time, you have a special Key-Value pair that represents who the leader is.\n\nThis is a tool like [[Zookeeper]]."},{"fields":{"slug":"/Engineering/Idempotent Operation/","title":"Idempotent Operation"},"frontmatter":{"draft":false},"rawBody":"An Operation that has the same ultimate outcome regardless of how many times it's performed. For example buying 5 times the same item should only charge the user once. Operations performed through a [[Publish-Subscribe Pattern|Pub/Sub]] messaging system typically have to be idempotent, since Pub/Sub systems tend to allow the same messages to be consumed multiple times..\n\n```mermaid \ngraph TD\nStart\nNS((\"New state\"))\n\nStart --\"apply method\"--> NS\nNS --\"apply method\"--> NS\n\n```"},{"fields":{"slug":"/Engineering/Jump hashing/","title":"Other strategy"},"frontmatter":{"draft":false},"rawBody":"# Other strategy\nSee [[Rendez-vous Hashing]]  \nSee [[Consistent hashing]]"},{"fields":{"slug":"/Engineering/Key-Value Stores/","title":"Tools"},"frontmatter":{"draft":false},"rawBody":"**Useful for caching or dynamic configuration.**  \nWhen you want to use a database that does not enforce relational format (in SQL).\n\nBecause we are accessing value directly thoruigh keys, you can access values very fast. Usually you have lower latency and increrased throughput.\n\n\n| Key | Value |\n|-----|-------|\n| foo | 9001           |\n| bar | SystemsExpert  |\n| baz | 1, two, 3      |\n\n# Tools\n- [[Redis]] (in memory storage)\n- Dynamo DB\n- [[ZooKeeper]]\n- [[Etcd]]\n"},{"fields":{"slug":"/Engineering/Leader Election/","title":"How to implement Leader election for your service ?"},"frontmatter":{"draft":false},"rawBody":"If you have a group of servers in charge of doing the same \"thing\" (like charging a user). `Leader Election` has the servers in question elect one of them as a `leader`, and that leader server is responsible to do this single action.  \nThe other servers (the `followers`) stays online, and if the `leader` goes down, a new leader is elected out of the `followers`.\n\nThis subject is hard because it is difficult to share the state of the knowledge of who the leader is. The act of getting `concensus` is hard.\n\n# How to implement Leader election for your service ? \nWe can use [[Etcd]]  (maybe Zookeeper) DB to implement leader election for our servers. You have multiple servers communicating with ETCD, and at any given point in time, you have a special Key-Value pair that represents who the leader is.  \nFor example the leader key could be `leader`. \n# Tools\n- [[Zookeeper]]\n- [[Etcd]]\n\n# Consensus Algorithm\n- Paxos \n- Raft\n"},{"fields":{"slug":"/Engineering/Load Balancer/","title":"How does the load balancer work ?"},"frontmatter":{"draft":false},"rawBody":"This is a server that can distributes loads between a bunch of servers.\n```mermaid \ngraph LR\nClient_1\nClient_...[...]\nClient_N\n\nLD((\"Load Balancer<br>(Reverse Proxy)\"))\nServer_1\nServer_...[...]\nServer_S\n\nClient_1 --> LD\nClient_... --> LD\nClient_N --> LD\n\nLD --> Server_1\nLD --> Server_...\nLD --> Server_S\n```\n\n# How does the load balancer work ?\nThere are Software vs hardware load balancer. The software has more customization and scaling capability. Here we will talk about software load balancer.\n\n## How does a LB has a new server to route to ?\nAdding a new server will register itself to the LB\n\n## How do we select the server to send to ? \n- Pure random redirection.\n- Round Robin approach: goes through server from top to bottom and come back to first one.\n- Weighted Round Robin: Still go through the first to the last server, but redirects more traffic or less depending on the weight. This would be useful if you have some more powerful servers \n- Using health checks, the LB can identify which server is struggling.\n- IP based LB: using a hash of the IP address of the client gets routed to a given server. This is useful when you have caching, because all the request of this client are going to the same server, this improves cache hits.\n- Cache based strategy: all the request related to a given path in the URL go to a given server. Useful if we want to deploy a big change to a given path, this change will only impact the servers that support this part of the path. All other services on the platform are unaffected in case of failure of deployment.\n\n\n## Using multiple LB\nIt makes sens to have multiple LB.\n\nFor example one LB using IP @, followed by LB following round robin\n\n```mermaid \ngraph LR\nClient_1\nClient_...[...]\nClient_N\n\nLD_IP((\"Load Balancer<br>(IP @ routing)\"))\n\nLD_RR1((\"Load Balancer<br>(Round Robin)\"))\nLD_RR2((\"Load Balancer<br>(Round Robin)\"))\n\nServer_1\nServer_2\nServer_3\nServer_4\n\nClient_1 --> LD_IP\nClient_... --> LD_IP\nClient_N --> LD_IP\n\nLD_IP --> LD_RR1\nLD_IP --> LD_RR1\nLD_IP --> LD_RR1\nLD_IP --> LD_RR1\n\nLD_IP --> LD_RR2\nLD_IP --> LD_RR2\nLD_IP --> LD_RR2\nLD_IP --> LD_RR2\n\n\n\nLD_RR1 --> Server_1\nLD_RR1 --> Server_1\nLD_RR1 --> Server_2\nLD_RR1 --> Server_2\nLD_RR2 --> Server_3\nLD_RR2 --> Server_3\nLD_RR2 --> Server_4\nLD_RR2 --> Server_4\n```\n\n## Redundant LB\nYou can have multiple load balancer to help in case of failure of a LB server. These servers communicate btw each other.\n\n# Tools\n**Nginx**"},{"fields":{"slug":"/Engineering/Latency and Throughput/","title":"Latency"},"frontmatter":{"draft":false},"rawBody":"Measure of performance of a system\n\n# Latency\nhow long it takes for data to go through a system. \n\t- network request  from client to server and back to client.\n\t- Time taken to read data:   \n\t\treading 1MB from memory: $2~\\micro\\text{seconds}$   \n\t\treading 1MB from SSD: $1000 ~\\micro \\text{ seconds}$  \n\t\treading 1MB from 1GB/s network : $10,000 ~\\micro \\text{ seconds}$  \n\t\treading 1MB from HDD : $20,000 ~\\micro \\text{ seconds}$  \n\t\tsend a packet from California to Netherlands and back : $150,000 ~\\micro \\text{ seconds}$\n\t\t\n```mermaid\ngraph LR\nClient\nServer\n\nClient --> Server\nServer --> Client\n```\n\n\n# Throughput\nHow much data can be send through the system over a given amount of time.\n\nHow to optimize for throughput: you can party for it to increase the number of servers.\n\n```mermaid\ngraph TD\nClient_1\nClient_2\nClient_3\n...\nClient_N\n\nServer\n\n\nClient_1 --> Server\nClient_2 --> Server\nClient_3 --> Server\n... --> Server\nClient_N --> Server\n```\n\n\nLatency and Throughput are not correlated."},{"fields":{"slug":"/Engineering/Logging and Monitoring/","title":"Logging"},"frontmatter":{"draft":false},"rawBody":"# Logging\n## Tools\n- Google Stack Driver\n\n\n# Monitoring\nUse a time series DB (Like influx DB, Graphite, Prometheus). And you have your server sending data to these DB.  \nYou can use tools like Graphana to connect to these DB and build graphs.\n\nYou can use monitoring to build alerts."},{"fields":{"slug":"/Engineering/MapReduce/","title":"Important points"},"frontmatter":{"draft":false},"rawBody":"MapReduce is based on the assumptions that most data processing tasks can be split up into a `map` and a `reduce` functions.\n\n```mermaid \ngraph LR\n\nAA(('AA'))\nAB(('AB'))\nAAC(('AAC'))\nBBB(('BBB'))\n\nKV1((\"k/v<br>pairs\"))\nKV2((\"k/v<br>pairs\"))\nKV3((\"k/v<br>pairs\"))\nKV4((\"k/v<br>pairs\"))\n\nN1((\"A\"))\nN2((\"B\"))\nN3((\"C\"))\n\nOutput((\"Output\"))\n\nAA --Map--> KV1\nAB --Map--> KV2\nAAC --Map--> KV3\nBBB --Map--> KV4\n\nKV1 --Shuffle--> N1\nKV2 --Shuffle--> N1\nKV2 --Shuffle--> N2\nKV3 --Shuffle--> N1\nKV3 --Shuffle--> N3\nKV4 --Shuffle--> N2\n\nN1 --Reduce--> Output\nN2 --Reduce--> Output\nN3 --Reduce--> Output\n\n```\n\nDo not forget, there is a `Shuffle` step between the Map and Reduce step !\n\n# Important points\n1. When dealing with MapReduce model, we assume we have a distributed file system. And the distributed FS has knowledge of where the data chunks reside and how to communicate with the machine that are going to apply the Map Operation (the `workers`).\n2. Because we have large dataset, we do not want to move the data, we move the Map operation (the map program) on the machines that contains the data.\n3. The Key-value pair after a Map operation is very important. The Reduce step relies on the Key to \"reduce\" / aggregate the data.\n4. MapReduce handles failures by re-doing a Map or Reduce operation. Hence our Map and Reduce functions are [[Idempotent Operation| idempotent]] !  (they do not change an external state)\n5. As an engineer dealing with a MapReduce job, what we care about is specifying Map and Reduce functions and their inputs / outputs.\n# Reference\n[MapReduce original white Paper](https://static.googleusercontent.com/media/research.google.com/en//archive/mapreduce-osdi04.pdf)\n"},{"fields":{"slug":"/Engineering/Peer-To-Peer Networks/","title":"Tools"},"frontmatter":{"draft":false},"rawBody":"A collection of machines referred to as peers that divide a workload between themselves to presumably complete the workload faster than would otherwise be possible. Peer-to-Peer networks often used in file-distribution systems.\n\nYou want to send a big file to many many machines. \n\nYou split this file into very small chunks. Send these chunks to all machines. Then let all the machines talks to each other to request between themselves the remaining chunks to build back the original file.\n\nThis is way better than going one by leveraging parallelism.\n\nHow do we know which peer to send a given chunk of the file ? \n- Using a `tracker`: You can use a DB that knows which server got which chunk so far.\n- Using the `gossip protocol` / `epidemic protocol`: servers talk between themselves and figure out what they need from each other. This relies on a [[Distributed Hash Table]] (DHT)\n\n# Tools\n- Kraken from Uber."},{"fields":{"slug":"/Engineering/Polling and Streaming/","title":"Polling"},"frontmatter":{"draft":false},"rawBody":"# Polling\n```mermaid \ngraph LR\nC1(\"Client 1<br>Polling\")\n\nS1(\"Server 1\")\n\nC1 -- \"Request<br>every x seconds\" --> S1\nS1 --> C1\n```\nHave a client issue a request at every time interval.\n\n# Streaming\n```mermaid \ngraph LR\nC2(\"Client 1\")\n\nS2(\"Server 2<br>Streaming\")\n\nS2 -.\"Streaming<br>continuously\".-> C2\n```\nThe Server opens a [[Socket]] on the client.  \nStreaming is pushing data from your server to the client. Whereas polling requires the client to initialize the data exchange with a request.\nStreaming allows to have instantaneous information flow without having many request being done by clients."},{"fields":{"slug":"/Engineering/Proxies/","title":"Forward Proxy"},"frontmatter":{"draft":false},"rawBody":"There are `Forward` and `Reverse` proxies. By default a proxy is a forward proxy.\n\n# Forward Proxy\nA Forward Proxy (FP) is a server that is between a client (or a set of clients) and another server (or a set of servers). A FP is a server that acts on behalf of client(s). A FP is on the client's side. If a client wants to communicate with a server, when a client issues a request to the server, it goes to the proxy and the proxy forwards the request to the server. The server gets the request from the FP, when the server responses, it replies to the proxy and the proxy sends the replies to the client.\n\nSo to the server, the server does not know the IP of the client, but the proxy only.\n\n```mermaid \ngraph LR\nClient\nFP[\"(Foward) Proxy\"]\nServer\n\nClient --1--> FP\nFP --2--> Server\nServer --3--> FP\nFP --4--> Client\n```\n\n# Reverse Proxy\nA Reverse Proxy (RP) acts on behalf of a Server. If a client wants to send a request to a server, if the RP is set up properly, the RP will get the requests (while the clients thinks its going to the server). \n\n```mermaid \ngraph LR\nClient\nFP[\"(Reverse) Proxy\"]\nServer\n\nClient --1--> FP\nFP --2--> Server\nServer --3--> FP\nFP --4--> Client\n```\n\nThe key thing is that the client **THINKS** that it is communicating with the server. This is the difference with FP.\n\nRP are very useful. For example a RP can filter out request you want to ignore. it can log / collect metrics, cache html pages. \none of a best use case is to do [[Load Balancer | load balancing]]."},{"fields":{"slug":"/Engineering/Publish-Subscribe Pattern/","title":"Tools"},"frontmatter":{"draft":false},"rawBody":"```mermaid \ngraph TD\nP1((P1))\nP2((P2))\nP3((P3))\n\nT1[[T1]]\nT2[[T2]]\nT3[[T3]]\n\nS1((S1))\nS2((S2))\nS2((S3))\n\nP1 --Push--> T1\nP2 --Push--> T2\nP3 --Push--> T3\n\nS1 --Pull--> T1\nS2 --Pull--> T1\nS3 --Pull--> T3\n```\n\n4 entities:\n- `Publishers`: Servers publishing data (in the form of messages) into Topics.\n- `Subscribers`: Clients that subscribes to Topics.\n- `Topics`: Conceptual Channels of specific information.\n- `Message`: Represents the data that is relevant to the Subscriber to get/process.\n\n\nThis is closely related to [[Polling and Streaming|Streaming]].\n\nA Pub/Sub system is like a DB. You are guaranteed at **least once** delivery to a subscriber. This can lead to issues where the same message is send multiple times to the same server. This issue enforces the subscriber to have [[Idempotent Operation]].\n\n### Additional property:\n- Messages are send to Subscribers in the same order as they come in. This is the behavior of a queue.\n- It is possible to \"replay\" messages.\n\n### Why do we need multiple topics ? \nPub/Sub is like a Database, which means that we may want to have multiple \"tables\" (as in SQL). So for each type of data, we want to have multiple topics. These topics have different sets of subscribers.\n\n# Tools\n- Apache Kafka\n- Google Cloud pub/sub: Topics scales automatically, sharding at a topic level is handled automatically.\n"},{"fields":{"slug":"/Engineering/Rate Limiting/","title":"Tools"},"frontmatter":{"draft":false},"rawBody":"The act of limiting the number of request sent to or from a system. Rate Limiting is most often used to limit the number of incoming requests in order to prevent DoS attacks and can be enforced at the IP-address level, at the user-account level or at the region level. Rate limiting can also be implemented in tiers; for instance, a type of network request could be limited to 1 per second, 5 per 10 seconds and 10 per minute.\n\n# Tools\nYou can use [[Redis]] to implement rate limiting by recording the number of request in this DB."},{"fields":{"slug":"/Engineering/Redis/","title":"Redis"},"frontmatter":{"draft":false},"rawBody":"An In memory Key-value store. Does offer some persistent storage options but is typically used as a really fast, best-effort caching solution.   \nRedis is also often used to implement [[Rate Limiting]]"},{"fields":{"slug":"/Engineering/Rendez-vous Hashing/","title":"Other strategy"},"frontmatter":{"draft":false},"rawBody":"We use a ranking to assign a client to a server. If a server goes down in a cluster, the ranking of server for a given client should be the same if the the top server is still running.\n\n# Other strategy\nSee [[Consistent hashing]]  \nSee [[Jump hashing]]  "},{"fields":{"slug":"/Engineering/Relational Databases/","title":"ACID"},"frontmatter":{"draft":false},"rawBody":"Data stored in relational DB are stored as tables.  \n**Most Relation DB supports SQL, which is a major reason why you want to use a Relational DB. Do not under estimate the importance of the query you want to be able to run on the DB.**\n\n# ACID\nSQL DB must support ACID transactions:\n- Atomicity: this dictates that multiple sub-operation in a command will all succeed or all fail.\n- Consistency: There are no stale stale in a DB where a future transaction does not know that a past transaction has executed.\n- Isolation: \n- Durability: The effect of a transaction are permanent (for example data stored on disk) \n\n# Database Index\nYou can create an auxiliary data structure in your DB that is optimize for a given query. \n\nThere are many indexes possible:\n- Bitmap indexes\n- Reverse indexes\n- Dense indexes\n\n### Example table\nFor example if we want to build an index related to the query to find all the customer that have the maximum amount:\n\n| customer_name | processed_at | amount |\n|---------------|--------------|--------|\n| Clement       | 2019-12-01   | 10     |\n| Antoine       | 2019-11-16   | 200    | \n| Simon         | 2020-02-02   | 9001   |\n| Meghan        | 2020-02-01   | 700    |\n\nWe can sort this DB by the Amount column in the index, and then use this new index to quickly answer this query.\n\nHaving an auxiliary DB will increase the memory cost and the write cost since we have to write to the original DB and the index.\n\nExample query for creating an index:\n```SQL\nCREATE INDEX table_idx_name on existing_table(column);\n```"},{"fields":{"slug":"/Engineering/Replication/","title":"Replication"},"frontmatter":{"draft":false},"rawBody":"> [!tldr] Replication\n> Keeping a copy of the same data on several different nodes, potentially in different locations. Replication provides **redundancy**: if some nodes are unavailable, the data can still be served from the remaining nodes. Replication can also help improve performance.\n> From Design Data-intensive apps\n\nThis mechanism goes hand in hand with [[Sharding or Partitioning|Sharding]].\n\nA system's performance is often only as good as its database's; optimize the latter, and watch as the former improves in tandem! \n\n```mermaid \ngraph LR\n\nMD((\"Main DB\"))\nReplica((Replica))\n\nMD --Sync-->Replica\nMD -.Async.-> Replica\n```\n\n# Replication\nIf the DB goes down, the system is down as well because we cannot read from DB. To solve for this, we can have a duplicate / replica of the original DB, this is a standby of the real DB. The main DB ensures the replica is up to date.\n\nHow to we keep the replica up to date ?   \n## Synchronous update\nWhenever there is an update of the main DB, the replica is updated synchronously. If the replica update fails, the write command fails for the main DB fails as well.\n\nWe can also use replication to improve the latency of the DB.\n\n## Async update\nIt is possible to update the replica asynchronously. This is ok if we do not need the replicas to be super up to date (like Linkedin posts).\n\n\n# References\n[Azure sharding doc](https://docs.microsoft.com/en-us/azure/architecture/patterns/sharding)\n[^1]: [[Ch 5 Replication]]\n\n"},{"fields":{"slug":"/Engineering/Security and HTTPS/","title":"Encryption"},"frontmatter":{"draft":false},"rawBody":"HTTP can be hacked with a Man In The Middle attack.\n\n# Encryption\n## Symmetric encryption\nRelies on symmetric key, so a single key, to both encrypt and decrypt data.\nDownside, that one key has to be shared between the 2 machines. This s hard to make sure that no one else get the key.\nThis requires AES (advanced encryption standards)\n\n## Asymmetric\nRelies on 2 keys, public and private  keys. When you encrypt a message using the public key, you can only decrypt it using the private key.\n\n# HTTPS\n## TLS (handshake)\nTransport Layer Security, which is a secured layer. So HTTPS is using TLS to be secure.  \nThe 2 services establishes a secure connection during the `TLS handshake`. \n\n```mermaid\nsequence\n```\n\n## SSL (certificate)\nSecure Socket Layer."},{"fields":{"slug":"/Engineering/Sharding or Partitioning/","title":"Sharding or Partitioning"},"frontmatter":{"draft":false},"rawBody":"> [!tldr] Partitioning\n> Splitting a big database into smaller subsets called partitions so that different partitions can be assigned to different nodes (also known as **sharding**).\n> From Design Data-intensive apps\n\nTo improve [[Latency and Throughput|throughput]] you can increase the replicas of a DB but this is limiting if the database if huge. So you may want to replicate specific subset of the data. Splitting the data across databases. Partitioning the data (this splitting) is known as **sharding**.\n\nHow do you know how to split the data and where to put it ?  \nFor example with tables, you can split up certain rows into different shards, for example by customer name.\n\n## Hotspots\nSome shards may get more traffic than others just by chance.  \nYou use hashing function to determine what shard a piece of data is gonna be written to and read from. [[Consistent hashing]] may be useful here, depending on the problem.\n\nThis logic of choosing how to do the sharding, you could implement it in the server that does the service itself. But in practice, this logic is implemented in a [[Proxies|Reverse Proxy]] that acts on behalf of the DB.\n\nThis mechanism goes hand in hand with [[Replication]].\n"},{"fields":{"slug":"/Engineering/Socket/","title":"Socket"},"frontmatter":{"draft":false},"rawBody":"A Socket is a way to have a long lived connection between two machines.\n\nA socket is like a \"portal\" to another machine without having to make repeated connection request.\n\n[[Polling and Streaming|Streaming]] is done using sockets.\n"},{"fields":{"slug":"/Engineering/Specialized Storage Paradigms/","title":"Blob Store"},"frontmatter":{"draft":false},"rawBody":"# Blob Store\nA BLOB means Binary Large Object.  \nA BLOB in an arbitrary of storage data. For example a video, image, large text, binary file.  \nA BLOB store is a storage solution for BLOBs.  \nA BLOB does not fit in an SQL DB.   \n\nBLOB stores are optimize for storing and retrieving massive amount of storage.\n\nThey behave like Key-Value store. Usually you access the data piece through some key. BUT they are not the same ! Because BLOB store is not optimize in the same way as Key-value. \n\n## Tools\n- GCS (Google Cloud Storage)\n- S3 (AWS)\n- Azure Blob storage (Microsoft)\n\n\n# Time Series DB\nIs a DB that is specialized to store Time Series data.  \nLike monitoring, telemetry data, sensors (like IoT), stock prices.\n\n## Tools\n- Influx DB\n- [[Prometheus]]\n\n# Graph DB\nA Graph DB][=]\n## Tools\n- Neo4j\n\n\n# Spatial DB + QuadTree\n## Tools\n- "},{"fields":{"slug":"/Engineering/Storage concept/","title":"Storage Concept"},"frontmatter":{"draft":false},"rawBody":"A Database is a server that handles saving and reading data from.\n`Persistence`: Outage may destroy data depending if the DB is using `disk` vs `memory`.\nReading data from memory is much faster than from disk.\n\nDifferent parameters for choosing a database: \n- Persistence: Disk vs Memory \n- Enforces structure on data ?\n- Availability: single vs distributed\n- Consistency: ACID, eventual consistency\n- Read / Write speeds\n"},{"fields":{"slug":"/Engineering/Zookeeper/","title":"Zookeeper"},"frontmatter":{"draft":false},"rawBody":"Zookeper is a strongly consistent, highly available [[Key-Value Stores]]. It's often used to store important configuration or to perform leader election.\n\n\nTool similar to [[Etcd]]."},{"fields":{"slug":"/ML/Bayesian AB testing/","title":"Tool"},"frontmatter":{"draft":false},"rawBody":"Why is it preferred over [[Frequentist AB testing]] ?\n- easier to interpret results. We don't have a p-value and a confidence interval, with Bayesian we can directly answer the question: what is the proba that B is better than experiment A.\n- Often fewer samples to each launch decision \n\t--> which means faster exp. \n\t--> which means faster improvements\n\t\n# Tool\n- Visual optimizer\n\n\n# Multi arm banding (MAB) vs AB\n![[Pasted image 20211220141202.png]]"},{"fields":{"slug":"/Finance/Growth Shares/","title":"Growth shares"},"frontmatter":{"draft":false},"rawBody":"Text bellow copied from: https://www.pinsentmasons.com/out-law/guides/growth-shares\n# Growth shares\n28 Jul 2021\n\n**Growth shares are a special class of shares issued to employees that allow the employees to share in the growth in value of the company above a valuation hurdle – usually on an exit event – in a tax efficient manner.**\n\nGrowth shares are typically granted by private, unlisted, companies in the UK, though they are sometimes issued by subsidiaries of AIM-listed companies too. Growth share can be used as an alternative to, or in conjunction with, EMI options. They can be a particularly attractive tool for recruiting and retaining staff in high-growth sectors such as technology and life sciences where they otherwise lack the financial clout of larger, more established businesses that pay larger salaries.\n\n## Characteristic of growth shares\nGrowth shares are a different class to the ordinary share capital and will typically confer no rights to dividends or voting. The shares will often be restricted to participation on an exit only basis, for example, on the sale of the company or an initial public offering.\n\nGrowth shares are subject to a 'hurdle' to encourage future growth of the company. If a company is currently worth £10 million, the growth shares could, for example, set a hurdle that states holders of growth shares will only share in any exit proceeds above £12 million.\n\nSince there is a possibility that the company will not achieve the growth required to benefit from the shares, growth shares can have a relatively low market value when issued to employees compared to ordinary shares. This makes it more affordable for employees to invest in the company.\n\nAs growth shares only allow their holders to share in the future growth of the company, there is no dilution for existing shareholders in respect of the current \"built-in value\" of the company at the date the growth shares are issued. \n\nGrowth shares can be used in conjunction with an Enterprise Management Incentive (EMI) plan and this can be particularly useful where the higher value of the ordinary shares means the company would struggle to make meaningful grants within the £250,000 individual limit. However, unlike EMI options which must be exercised within 10 years of being granted, growth shares do not have a time limit and may therefore be a suitable alternative for companies that do not expect to exit in the near to medium future.\n\nVesting and leaver provisions can apply to the growth shares so in effect the arrangements commercially mirror a market value option arrangement.\n\n## Tax treatment of growth shares\nThe tax treatment of a subscription for growth shares is best illustrated by way of example: say a company is worth £10 million at the time of issue of the growth shares and an employee subscribes for shares that gives the employee 1% of the company value above a hurdle of £12 million. In this example, the growth shares have been valued at £1,000 at the date of subscription and the employee pays the full market value for the shares which means that there is no tax or National Insurance contributions (NICs) to pay on issue. If the company is subsequently sold for £20 million, the value of the growth shares will be £80,000 – this being 1% of £20 million less £12 million. The employee's gain of £79,000 – the £80,000 exit value less the £1,000 paid for the shares at the outset – will be subject to capital gains tax (CGT). \n\nIf, rather than pay for the £1,000 market value of the shares in the above example, the employee is issued the shares at a discount, in addition to the CGT on disposal of the growth shares, the discounted element would be subject to income tax and possibly NICs too at the date of issue of the growth shares. \n\nThe market value of a growth share cannot be agreed with HM Revenue & Customs (HMRC) so it is vital that the company conducts a robust valuation and maintains accurate records to justify the valuation in the event it is challenged by HMRC in the future.\n\nGrowth share plans can therefore be a form of tax efficient incentive for employees with low acquisition costs and CGT treatment on growth. However, this will only remain the case if:\n\nthere are no changes in the tax legislation to impose income tax on the growth in value of growth shares; and\nCGT rates remain lower than income tax rates, especially for higher rate income taxpayers.\nEmployees should therefore be told of the risk that the tax treatment of the growth shares may change after acquisition.\n\n## Advantages and disadvantages of implementing a growth share plan\nThe advantages include the following:\n- shares are issued to employees upfront with acquisition costs usually low;\n- there is no dilution for existing shareholders in respect of the current company valuation, which could reassure current shareholders whilst simultaneously incentivising key employees to promote the growth of the company; and\n- growth shares do not expire after 10 years, unlike EMI options, so growth shares could benefit employees in companies that do not envisage an exit in the near to medium future.\n\n\nThe disadvantages include the following:\n- as shares are issued to employees upfront, employees will need to pay for the growth shares at the time of award. If the employee subscribes for the growth shares at a discount to market value, they will need to pay income tax, and possibly NICs, on the amount of the discount;\n- HMRC will not agree a valuation for growth shares, unless used in conjunction with an EMI option plan, so a robust valuation should be completed by the company each time growth shares are awarded. This will add to the costs of operating a growth share plan and introduces an element of uncertainty as there is no guarantee that HMRC will subsequently accept the share valuation reached by the company; and \n- changes to the articles of association of the company will be necessary to create a new class of growth shares.\n\n\n## Setting up a growth share plan\nThe following steps are usually taken to set up a growth share plan:\n\ndraft amendments to the articles of the company to create a new class of shares – the growth shares;\nobtain shareholder agreement to amend the company’s articles;\nestablish the growth share plan;\nobtain a valuation for the growth shares;\nenter into an agreement – a growth share subscription agreement – with the relevant employees; and\nissue the growth shares to the employee.\n\n## Conclusion\nGrowth shares arrangements can be relatively straightforward and are an attractive alternative to non-tax-advantaged share options or where ordinary shares in the company would have a high market value, and the employee would be unwilling or unable to pay the full market value on acquisition. Companies that find themselves unable to grant qualifying EMI options or are struggling to stay within the individual EMI limits should consider using growth shares to incentivise employees.\n\n"},{"fields":{"slug":"/ML/Constrained optimization/","title":"Constrained Optimization"},"frontmatter":{"draft":false},"rawBody":"We wish to find the maximal or minimal value of $f(\\boldsymbol{x})$ for values of $\\boldsymbol{x}$ in some set $\\mathbb{S}$. This is known as constrained optimization. Points $\\boldsymbol{x}$ that lie within the set $\\mathbb{S}$ are called feasible points in constrained optimization terminology.\n\n- We can do gradient optimization with a small step size. Then project the obtained set of parameters back in the set $\\mathbb{S}$.  \n- We could do something similar with [[Line Search]] where we only considered feasible values $\\boldsymbol{x} \\in \\mathbb{S}$. \n- More sofisticated, the [[KKT]] approach."},{"fields":{"slug":"/ML/Coordinate descent/","title":"Coordinate Descent"},"frontmatter":{"draft":false},"rawBody":"[Coordinate descent](https://en.wikipedia.org/wiki/Coordinate_descent) is an optimization algorithm that successively minimizes along coordinate directions to find the minimum of a function. At each iteration, the algorithm determines a coordinate or coordinate block via a coordinate selection rule, then exactly or inexactly minimizes over the corresponding coordinate hyperplane while fixing all other coordinates or coordinate blocks. A [[Line Search]] along the coordinate direction can be performed at the current iterate to determine the appropriate step size. Coordinate descent is applicable in both differentiable and derivative-free contexts.\n\n### **Coordinate descent is applicable in both differentiable and derivative-free contexts.**\n\n\n![[Pasted image 20211213144445.png|500]]"},{"fields":{"slug":"/ML/Determinant/","title":"Determinant"},"frontmatter":{"draft":false},"rawBody":"$det(\\boldsymbol{A})$ maps a matrix to a scalar.  \n- The `determinant` is equal to the product of all eigenvalues of a matrix.\n- The absolute value of the determinant can be thought of as a measure of how much multiplication by the matrix expands or contracts space. \n"},{"fields":{"slug":"/ML/Dirichlet distribution/","title":"Dirichlet Distribution"},"frontmatter":{"draft":false},"rawBody":"Exponential Family distribution over the Simplex (positive vectors that sums to 1).   \nA Dirichlet is parameterize by a K-1 vector named $\\alpha$. ($\\alpha$ does not sum to one, this is a parameter not a probability.)\n\nThe Dirichlet is a conjugate to the multinomial. \n\n- $E[\\theta_i | \\alpha] = \\frac{\\alpha_i}{\\sum\\alpha_i}$, ie the mean.\n- $\\sum \\alpha_i$ determine the peakiness of the Dirichlet, ie the scaling.\n\n##### What happens when $\\alpha < 1$ or equivalently $s < K$? \nInstead of having a \"peak\" in the dirichlet triangle, you get a \"bowl\" kind of, where the edges have higher values.\nThe samples categories get \"sparser\".\n\n\n##### Conjugacy\n$\\theta \\sim Dir(\\alpha)$ and  $Z_n \\sim Mult(\\theta)$, so what is conditional distribution $p(\\theta|Z_{1..N})$ ?\n\nLet $n(Z_{1..N})$ be counts of each atom (atom ?). then $\\theta|Z_{1..N} \\sim Dir(\\alpha + n(Z_{1..N}))$\n\n\n![[Pasted image 20211111134057.png]]"},{"fields":{"slug":"/ML/Eigendecomposition/","title":"Eigendecomposition"},"frontmatter":{"draft":false},"rawBody":"Decomposition of matrix into a set of `eigenvectors` and `eigenvalues`.\nAn `eigenvector` of square matrix $\\boldsymbol{A}$ is a non-zero vector $\\boldsymbol{v}$ such that multiplication by  $\\boldsymbol{A}$ alters only the scale of  $\\boldsymbol{v}$.\n$$\\boldsymbol{A} \\boldsymbol{v} = \\lambda \\boldsymbol{v}$$\n$\\lambda$ is the `eigenvalue` corresponding to this eigenvector.  \nConcatenating all eigenvectors per column into a matrix  $\\boldsymbol{V}$ and all eigenvalues into a vector  $\\boldsymbol{\\lambda}$.  \nThe `eigendecomposition` of  $\\boldsymbol{A}$   is then:\n$$ \\boldsymbol{A} =  \\boldsymbol{V} diag(\\boldsymbol{\\lambda})\\boldsymbol{V}^{-1}$$\nEvery real [[Symmetric matrix]] can be decomposed into an expression using only real-valued eigenvectors and eigenvalues: $\\boldsymbol{A} = \\boldsymbol{Q}\\boldsymbol{\\Lambda}\\boldsymbol{Q}^T$ where $\\boldsymbol{Q}$ is an orthogonal matrix composed of eigenvectors of $\\boldsymbol{A}$, and $\\boldsymbol{\\Lambda}$ is a diagonal matrix.\n\n##### A matrix is [[Singular matrix|singular]] iff any of the eigenvalues are zero.\n\nA matrix with all positive eigenvalues is called [[Positive Definite Matrix|positive definite]].  \nA matrix with all positive of zero-valued eigenvalues is called [[Positive Definite Matrix|positive semidefinite]].  \nA matrix with all negative eigenvalues is called `negative definite`.  \nA matrix with all negative of zero-valued eigenvalues is called `negative semidefinite`.\n"},{"fields":{"slug":"/ML/Feature hashing/","title":"Feature Hashing"},"frontmatter":{"draft":false},"rawBody":"Feature hashing, called the hashing trick, converts text data or categorical attributes with high cardinalities into a feature vector of arbitrary dimensionality.\n\n### Benefits\n\n-   Feature hashing is very useful for features that have high cardinality with hundreds and thousands of unique values. Hashing trick is a way to reduce the increase in dimension and memory by allowing multiple values to be present/encoded as the same value.\n\n### Feature hashing example\n\n-   First, you chose the dimensionality of your feature vectors. Then, using a hash function, you convert all values of your categorical attribute (or all tokens in your collection of documents) into a number. Then you convert this number into an index of your feature vector. The process is illustrated in the diagram below.\n\nfox3brown4quick4the0\n\nAn illustration of the hashing trick for desired dimensionality of 5 for the originality of K of values of an attributes\n\n-   Let’s illustrate what it would look like to convert the text “The quick brown fox” into a feature vector. The values for each word in the phrase are:\n    \n    ```\n    the = 5\n    quick = 4\n    brown = 4\n    fox = 3\n    ```\n    \n- Let define a hash function, hh, that takes a string as input and outputs a non-negative integer. Let the desired dimensionality be 5. By applying the hash function to each word and applying the modulo of 5 to obtain the index of the word, we get:\n    ```\n    h(the) mod 5 = 0h(quick) mod 5 = 4h(brown) mod 5 = 4h(fox) mod 5 = 3\n    ```\n\n-   In this example:\n\t- `h(the) mod 5 = 0` means that we have one word in dimension **0** of the feature vector.\n\t- `h(quick) mod 5 = 4` and `h(brown) mod 5 = 4` means that we have two words in dimension **4** of the feature vector.\n\t- `h(fox) mod 5 = 3` means that we have one word in dimension **3** of the feature vector.\n\t- As you can see, that there are no words in dimensions 1 or 2 of the vector, so we keep them as 0.\n\n-   Finally, we have the feature vector as: `[1, 0, 0, 1, 2]`.\n-   As you can see, there is a collision between words “quick” and “brown.” They are both represented by dimension 4. The lower the desired dimensionality, the higher the chances of collision. To reduce the probability of collision, we can increase the desired dimensions. This is the trade-off between speed and quality of learning.    \n\n> Commonly used hash functions are [MurmurHash3](https://en.wikipedia.org/wiki/MurmurHash), [Jenkins](https://en.wikipedia.org/wiki/Jenkins_hash_function), [CityHash](https://en.wikipedia.org/wiki/List_of_hash_functions), and [MD5](https://en.wikipedia.org/wiki/MD5).\n\n### Feature hashing in tech companies[#](https://www.educative.io/courses/machine-learning-system-design/q2AwDN4nZ73#Feature-hashing-in-tech-companies)\n\n-   Feature hashing is popular in many tech companies like [Booking](https://www.booking.com/), [Facebook](https://www.facebook.com/), [Yahoo](https://www.yahoo.com/), [Yandex](https://yandex.com/), [Avazu](http://avazu.com/home/) and [Criteo](https://www.criteo.com/).\n-   One problem with hashing is collision. If the hash size is too small, more collisions will happen and negatively affect model performance. On the other hand, the larger the hash size, the more it will consume memory.\n-   Collisions also affect model performance. With high collisions, a model won’t be able to differentiate coefficients between feature values. For example, the coefficient for “User login/ User logout” might end up the same, which makes no sense.\n\n> Depending on application, you can choose the number of bits for feature hashing that provide the correct balance between model accuracy and computing cost during model training. For example, by increasing hash size we can improve performance, but the training time will increase as well as memory consumption."},{"fields":{"slug":"/ML/Frequentist AB testing/","title":"Frequentist A/B testing"},"frontmatter":{"draft":false},"rawBody":"Form an Hypothesis:\n- Replace a user experience with another\n- Dependent variable selection: eg the profit lift brought by the new change, increased click through rate, screen time, ...\n- Directionality of these dependent variables: identify the change of each dependent variables\n- Experiment participants\n\nTemplate: \n`\"If we replace X with Y for some distincts set of user, then [A, B, C] will go [up/down] and [invariants] won't change.\"`\n`[A, B, C]` are the dependent variables.\n\nsee [[Bayesian AB testing]]\n\n# Frequentist A/B testing\nControl group in which the experience is unchanged so that we can compare a treatment group. \n-> Determine if the treatment group should replace control group.\n\n\nRun for at least 2 weeks.\n### Results extrapolation\n- Through time\n- Through population\n\n### Impact of change:\n- novelty effect: user uses the new UI more because it's new, not because it's better\n- Change aversion: user don't use the new UI because it's different\n- Time intensive feedback: sometime, the chosen dependent variable we want to optimize takes too long to get. For example think graduation grades for master, if you change the first year course, it will take 4 years to get feedback. SUch a long period of time will surely have invariante variable change which is no good. -> we want feedback within a month\n\n# A/A testing\nTo measure that our dependent variables don't have crazy variance, and maybe to measure the `minimum detectable change` required to reject the null hypothesis, we can do A/A testing, which is running the same platform for 2 sets of users and measuring the p-value, etc of the hypothesis testing we will do in A/B testing. We expect to see that the null hypothesis is not rejected in this case.\n\n\n# Tools for A/B testing\n- Optimizely \n- Google optimize\n- Facebook plan out\n\n\n# Multi arm banding (MAB) vs AB\n![[Pasted image 20211220141202.png]]"},{"fields":{"slug":"/ML/GNN/","title":"Graph Neural Network"},"frontmatter":{"draft":false},"rawBody":"# Graph Neural Network\n\n# Good videos \n[Geometric Deep Learning: GNNs Beyond Permutation Equivariance](https://www.youtube.com/watch?v=aCUOAkOqNoU) Explaining why all NN can be considered GNNs.\n\n"},{"fields":{"slug":"/ML/Gradient/","title":"Gradient"},"frontmatter":{"draft":false},"rawBody":"The Gradient is the extension of derivative to the case where the derivative is with respect to a vector.  \nThe gradient of f is the vector containing all of the partial derivatives: \n$$\\nabla_x f(\\boldsymbol{x})$$\n\n- $f : \\mathbb{R} \\rightarrow \\mathbb{R}$: derivative\n- $f : \\mathbb{R}^n \\rightarrow \\mathbb{R}$: Gradient $\\nabla_x f(\\boldsymbol{x}) \\in \\mathbb{R}^n$.\n- $f : \\mathbb{R}^m \\rightarrow \\mathbb{R}^n$: [[Jacobian]] $\\boldsymbol{J} \\in \\mathbb{R}^{n \\times m}$"},{"fields":{"slug":"/ML/Handle Imablanced datasets/","title":"Oversampling"},"frontmatter":{"draft":false},"rawBody":"Usual ML algorithms are designed to operate on classification data with an equal number of observations for each class. \n\nSome models use prior probabilities, such as [[Naive Bayes Classifier]] and Discriminant Analysis Classifiers. Unless specified manually, these models typically derive the value of the priors from the training data. Using more balanced priors or a balanced training set may help deal with a class imbalance.\n\nExamples here are given for binary classification task, but can be extended for multi-class.\n\nThere are a few ways to compensate for imbalanced datasets\" \n- Oversampling\n- Undersampling\n- Loss weighting\n\n\n# Oversampling\nGenerate new training sample of the minority class. \n1. Random Oversampling: duplicates minority samples randomly\n1. Synthetic Minority Oversampling technique (**SMOTE**): popular. Produce new samples of minority class by sampling along the line defined by 2 samples.\n1. Many extensions of SMOTE\n\n# Undersampling\nReduce the training set size.\n1. Random undersampling: randomly delete samples from Majority class\n2. **Tomek Links**: Tomek links is found when 2 samples are nearest neighbour of each other and have different classes. The majority one will be removed.\n\n\n> It is beneficial to combine Oversampling and Undersampling. For example: SMOTE + Tome Links is popular.\n\n\n# Loss Weighting\nFor Random Forest or Neural networks, it is also possible to modify the loss to increase the contribution of the minority class. \nThe weighted Cross entropy loss becomes: $CE(y, \\hat y) = \\sum_0^N w_{y=1} \\cdot y log(\\hat{y}) + w_{y=0} \\cdot (1 - y) log(1 - \\hat{y})$\nFor the random Forest, the Gini index can be modified."},{"fields":{"slug":"/ML/Hessian/","title":"Hessian"},"frontmatter":{"draft":false},"rawBody":"The hessian is the collection of second derivative of the gradient. ie, the hessian is the [[Jacobian]] of the [[Gradient]].\n\n$$\\boldsymbol{H}(f)(\\boldsymbol{x})_{i, j} = \\frac{\\partial^2}{\\partial x_i \\partial x_j} f(\\boldsymbol{x})$$"},{"fields":{"slug":"/ML/Jacobian/","title":"Jacobian"},"frontmatter":{"draft":false},"rawBody":"The `Jacobian` is the extension of a gradient where the partial derivaties of a function whose input and output are both **vectors**.\n\nGiven a function $\\boldsymbol{f}: \\mathbb{R}^{m} \\rightarrow \\mathbb{R}^{n}$, its Jacobian  $\\boldsymbol{J} \\in \\mathbb{R}^{n \\times m}$ is defined as $J_{i,j}  = \\frac{\\partial}{\\partial x_i} f(\\boldsymbol{x})_i$"},{"fields":{"slug":"/ML/KKT/","title":"KKT conditions"},"frontmatter":{"draft":false},"rawBody":"The **Karush–Kuhn–Tucker** (KKT) approach provides general solution to [[Constrained optimization]]. With the KKT approach, we introduce a new function called the `generalized Lagrangian`.\n\n# KKT conditions\nThese conditions are necessary, but not always sufficient, for a solution point to be optimal for the given generalized Lagrangian.\n- The gradient of the generalized Lagrangian is zero.\n- All contraints on both $\\boldsymbol{x}$ and the KKT multipliers are satisfied.\n- The inequality constraints exhibit \"complementary slackness\": $\\alpha \\odot \\boldsymbol{h}(\\boldsymbol{x}) = 0$"},{"fields":{"slug":"/ML/Kernel Trick/","title":"Kernel Trick"},"frontmatter":{"draft":false},"rawBody":"Well explained on page 11 of `Deep Learning` book by Ian Goodfellow, Yoshua Bengio, Aaron Courville. (in my paperpile or google drive)."},{"fields":{"slug":"/ML/Latent Dirichlet Allocation/","title":"Links"},"frontmatter":{"draft":false},"rawBody":"\nThis can be used for Topic Modeling.\n\nExtract topics from multiple documents. This is an unsupervised approach.\nLDA is a probabilistic model: \n- Treat data as obs that arise from a generative prob process that includes hidden variables\n\t- For Documents, the hidden variables reflect the thematic structure of the collection\n- Infer the hidden structure using posterior inference\n\t- What are the topics that describe this collection ? \n- Situate new data into the estimated model\n\t- How does this query or new document fit into the estimated topics.\n\nIn LDA, the intuition is that **Documents exhibit multiple topics**.\n\nThe underlying model of how a document is generated is based of: \n- Every topic **is** a distribution over a fixed vocab. \n- We have a distribution over the topics.\na document is a given distribution over the topics, then we sample topics, and then we sample one word for the sampled topic.\n\nGraphical model:\n![[Pasted image 20211111132355.png|700]]\n\nHow do we infer this model ? (the latent variables)\n\n##### Q: Why do we condition on $Z$ and $\\beta$ ? Why do we have a condition for W over ALL $\\beta_K$ and not just the beta for the given topic ?\n\n$p(W_{d,n} | Z_{d,n}, \\beta_K)$ because $Z_{d,n}$ is a topic index, using this topic index we can select this topic distribution from $\\beta_K$.\n\n##### Q: mixed membership model\nsee video in link below at ~1h5min\n\n# Links\n[Nice Lecture from David Blei](http://videolectures.net/mlss09uk_blei_tm/)\n"},{"fields":{"slug":"/ML/Line Search/","title":"Line Search"},"frontmatter":{"draft":false},"rawBody":"[Line search](https://en.wikipedia.org/wiki/Line_search) In optimization, the line search strategy is one of two basic iterative approaches to find a local minimum  $\\mathbf {x} ^{*}$ of an objective function $f:\\mathbb {R} ^{n}\\to \\mathbb {R}$ . The other approach is [[trust region]].\n\nThe line search approach first finds a descent direction along which the objective function {\\displaystyle f}f will be reduced and then computes a step size that determines how far $\\mathbf {x}$  should move along that direction. The descent direction can be computed by various methods, such as gradient descent or quasi-Newton method. The step size can be determined either exactly or inexactly."},{"fields":{"slug":"/ML/Matrix inverse/","title":"Matrix Inverse"},"frontmatter":{"draft":false},"rawBody":"A matrix has an inverse if it is square and all columns are linearly independent.  \nA square matrix that is not invertible is called `singular` or `degenerate`."},{"fields":{"slug":"/ML/Moore-Penrose Pseudoinverse/","title":"Moore-Penrose Pseudoinverse"},"frontmatter":{"draft":false},"rawBody":"Matrix inversion is not defined for matrices that are not square. \nTo get the pseudo inverse of a matrix $\\boldsymbol{A}$, we use the [[Singular Value Decomposition (SVD)|SVD]] of $\\boldsymbol{A}$. Then the pseudo inverse of $\\boldsymbol{A}$ is:\n\n$$\n\\boldsymbol{A}^+ = \\boldsymbol{V} \\boldsymbol{D}^+\\boldsymbol{U}^T\n$$\n\n"},{"fields":{"slug":"/ML/Multi-armed bandit (AB testing)/","title":"Exploitation vs exploration"},"frontmatter":{"draft":false},"rawBody":"Minimize the negative business impact of the B experiement while still experimenting at a reasonable pace.\n\n> note: it is not a good idea to just to regular AB testing with a smaller proportion of pop doing the B exp. Because in the need we need a significant amount of people doing the B exp in order to reject or not the hypothesis. so routing 99% to exp A and 1% to exp B is practically the same as doing 50/50. \n\n\nSo during the 14 day trials of this AB, we measure the performance at 7 days. and how do we choose to route customers more efficiently given this partial experiemnt so far ? \n\n# Exploitation vs exploration\n## Epsilon greed strategy\nHow much better is this epsilon greed vs regular AB testing. \n\n# Thomson sampling\nThe frequency a user should be allocated to an experiment should equal the probability of that experience being optimal.\n\n\n# Multi arm banding (MAB) vs AB\n![[Pasted image 20211220141202.png]]\n\n\n# Tool\n- Optimizely\n- Visual web optimizer\n- Vowpal Wabbit (contextual bandit)"},{"fields":{"slug":"/ML/Naive Bayes Classifier/","title":"The ultimate understanding of the Naive Bayes Classifier, version 8"},"frontmatter":{"draft":false},"rawBody":"Given a corpus of text and their label as training set, we want to build a classifier that can give us a class given a text.  \nMore formally, given a vector of multiple discrete values features $\\boldsymbol x \\in \\{1, ...K\\}^D$ and a target label $y$. \n\n$$\n\n\\begin{aligned}\n\np(y=c | \\boldsymbol x) & =  \\frac{p(y=c)p(\\boldsymbol x|y=c)}{p(\\boldsymbol x)} \\\\\n& = \\frac{p(y=c)\\prod_{i=1}^D p(x_i|y=c)}{p(\\boldsymbol x)} \\Leftarrow \\textit{ naive assumption of conditional independence}\n\\end{aligned}\n$$\n\nFor example, $\\boldsymbol x$ is a vector representing a document.\n1. If $x_i$ represents the absence or presence of $word_i$ in a text. In this case, $K = 2$ because $x_i \\in \\{0, 1\\}$\n2. If $x_i$ represents the frequency (count) of $word_i$ in a text, then $x_i$ can have any discrete value from 0 to infinity. So $x_i \\sim Bin(\\mu_i)$. A Binomial distribution of the frequency to be 0, 1, 2, 125, ... So $\\mu_i$ is a vector of size $N$ which denotes the maximum frequency we have from the training set.   \n   /!\\ I used to confuse myself. This is not a CATEGORICAL distribution but a Multinomial !!!!!!!! If we see the Document containing $n$ words, each word could be from a vocab of size $k = |vocab|$ then what is the probability that we have $word_1$ appearing 10 times, $word_3$ 2 times, ... This distribution totally makes sense for any number of word frequency, even ones that we not present during training. For example,. this is the first time we see $word_8$ appearing 123 times.\n   \n3. It is possible to handle other type of distribution for $x_i$ even different distributions for every features.\n\n### Directed Graphical Model\n![[Pasted image 20211215205658.png|200]] ^8d39ff\n\nWhere:\n- $\\boldsymbol \\pi$ is the prior of the categorical distribution of $Y$: $Y \\sim Cat(\\pi)$\n- $x_{ij}$ is the feature of the jth word for the ith document. $j \\in \\{1, ..., D\\}$ words (ie D is the vocab size) and $i \\in \\{1, ... N\\}$ documents. \n- $\\theta_{jc}$ is the parameter of the $x_{ij}$ feature variable. The document $i$  is associated with a label $y_i$ which has values in the possible class $C$. \n- The plates: there are $N$ documents, $D$ words across all N documents (vocab), there are $C$ possible outcome class values for $Y$. \n\n\n\n# The ultimate understanding of the Naive Bayes Classifier, version 8\n\nNotations: \n- $\\mathbf{x} = \\{x_1, ..., x_D\\}$ the feature vector for one sample. Each $x_j$ is a random variable.\n- $\\mathcal{D} = \\{(\\mathbf{x}_i, y_i)\\}^N$ is the training set which is different from $D$ the number of features. (convention from Machine Learning a probabilistic Perspective, not mine)\n- $\\theta$ are model's parameters.\n\nWe would like to classify vectors of discrete-valued features, $\\mathbf{x} \\in \\{1, ... K\\}^D$ where $K$ is the number of values for each feature, and $D$ is the number of features.   \nAfter training on the dataset $\\mathcal{D}$, the goal of the prediction is to find the class $y$ given the feature $\\mathbf{x}$: $$\\hat{y} = \\underset{y\\in\\{1, .., C\\}}{max} ~ p(y=c|x, \\mathcal{D})$$We will use the generative approach (**TODO: Explain why this is a generative approach**). \nUsing Bayes rule we have: $$p(y=c|x, \\mathcal{D}) \\propto  p(y=c | \\mathcal{D})p(\\mathbf{x} | y = c, \\mathcal{D})$$\nLet's introduce the \"naive\" assumption made in the Naive Bayes model. We consider each features to be independent from each other. So the conditional probability simplifies into:\n$$p(\\mathbf{x}|y=c, \\theta) = \\prod_{j=1}^D p(x_j|y=c, \\theta_{jc})$$\nWhere $\\theta$ are the parameters of the model and $\\theta_{jc}$ are the parameters describing the probability distribution of the independent variable $x_i$.\n\nHere is the Directed Graphical model representation of the NBC. The random variable $y$ is not observed (not shaded), but the variables $x_j$ are observed (shaded). We have $D$ features.\n![[Pasted image 20220215201300.png | 600]]\n\n## The input data\nSo far, we have not made assumption about our data points $\\mathbf{x}$. We have the option to choose our models to suit the type of each feature:\n- In the case of real-valued features, we can use the Gaussian distribution: $p(\\mathbf{x} | y=c, \\theta) = \\prod_{j = 1}^D \\mathcal{N}(x_j | \\mu_{jc}, \\sigma_{jc}^2)$ where $\\mu_{jc}$ is the mean of the feature $j$ in objects of class $c$, and $\\sigma_{jc}^2$ is its variance.\n- In the case of binary features, $x_j \\in \\{0, 1\\}$, we can use the Bernoulli distribution: $p(\\mathbf{x} | y=c, \\theta) = \\prod_{j = 1}^D Ber(x_j | \\mu_{jc})$ where $\\mu_{jc}$ is the probability that feature $j$ occurs in class $c$. This is called the **multivariate Bernoulli naive Bayes model**\n- In the case of categorical features, $x_j \\in \\{1, K\\}$ we can use the multinoulli distribution:  $p(\\mathbf{x} | y=c, \\theta) = \\prod_{j = 1}^D Cat(x_j | \\mu_{jc})$ where $\\mu_{jc}$ is the histogram over the $K$ possible values for $x_j$ in class $c$. \n- In the case of positive integers features, $x_j \\in \\{0, 1, ..., \\inf \\}$,we can consider these values as counts. We can use the Multinomial distribution: $p(\\mathbf{x} | y=c, \\theta) = Mul(\\mathbf{x} | \\boldsymbol{\\mu}) = \\frac{n!}{x_1!...x_k!}p_1^{x_1}\\times ... \\times p_k^{x_k}$ where $n= \\sum_{j=1}^D x_j$ \n\n**Example features:**\nMost common example is classifying text. What would be $D$ in this case ? \n\nCan $D$ be the number of words in a document ? Then each document has an $\\mathbf{x}$ of different dimensions. Then it would be hard to fit such a model. If each variable $x_j$, $j \\in \\{1, D\\}$ would have different amount of training data. Especially, if only one document is much larger than the others, then the last variables $x_i$ would have their distribution parameters fitted based on a single example from that documents. In addition, for this approach, the word order does not matter so if the words is last or first, its probability should be the same. **So no $D$ is not the size of a document.** \n\n-> If we choose $D$ to be the number of different words across all our documents, ie $D = |vocab|$ is the vocabulary size. Then the order of the words does not matter, which is good, and each $x_j$ can be adapted depending on our data representation.\n\nDoc 1 = \"This morning, this is BBC radio.\", Doc 2 = \"This is a rabbit and a Dog\".\n\n|format |sentence | \"morning\" | \"this\" |  \"is\"  |  \"a\"  | \"rabbit\" | \"and\" | \"dog\" | \"BBC\" | \"radio\" |\n|-------|----------|----------|-----|----|----|-------|----|------|----|----|\n|binary |Doc1 | 1 | 1 |  1  |  0  | 0 | 0 | 0 | 1 | 1|\n|binary |Doc2 | 0 | 1 |  1  |  1  | 1 | 1 | 1 | 0 | 0|\n|counts |Doc1 | 1 | **2** |  1  |  0  | 0 | 0 | 0 | 1 | 1|\n|counts |Doc2 | 0 | 1 |  1  |  **2**  | 1 | 1 | 1 | 0 | 0|\n\n\n\n ## Model fitting\n We now get into finding the best parameters $\\theta$ of our model. We solve for $\\theta$ considering that our features are Bernoulli distributed. See the section **TODO** for the solution with Multinoulli distributed features.\n\n ### Maximum Likelihood estimate for NBC\n A measure of fit is the famous **likelihood** function. The likelihood is the probability of our training data given the parameters $\\theta$. \n $$Likelihood(\\mathcal{D} | \\theta) = p(\\mathbf{x}_i, y_i | \\theta)$$ where  $(\\mathbf{x}_i, y_i)$ is a sample from our training set. \nUsing this measure of fit, we want to find the best $\\theta$, which means maximising the likelihood function with regards to $\\theta$. \nSimilarly, and for convenience later on, we will maximize the **log likelihood** of the data. This is very useful as the $log$ converts multiplications into summations. This is useful to sort out the math but also in practice to avoid underflows (since we multiply probabilities). \n\n The probability for a single data case is given by\n $$p(\\mathbf{x_i, y_i} | \\boldsymbol{\\theta}) = p(x_i | \\boldsymbol{\\pi}) \\prod_{j = 1}^D p(x_{ij} | \\boldsymbol{\\theta}_j) = \\prod_c^C \\pi_c^{\\mathbb{I}(y_i = c)} \\prod_j^D \\prod_c^C p(x_{ij} | \\theta_{jc})^{\\mathbb{I}(y_i = c)}$$ Reminder, $\\mathbb{I}(y_i = c)$ is 1 when $y_i = c$  or 0 if not. The right most term tells us that the parameters of the feature distribution depends on $y$.\n Hence the log-likelihood over the whole training set $\\mathcal{D}$ is given by:\n $$log(p(\\mathcal{D} | \\boldsymbol{\\theta})) = \\sum_{c=1}^C N_c log(\\pi_c) + \\sum_{j=1}^D\\sum_{c=1}^C\\sum_{i:y_i=c} log~p(x_{ij}|\\theta_{jc})$$\nWhere $N_c$ is the number of documents of class $c$. Maximizing the log-likelihood can be done by maximizing the 2 sums independently, one with $\\pi$ and one with $\\theta_{jc}$.\n**The solution to the MLE is then:** $$\\hat \\pi = \\frac{N_c}{N} \\text{  and  } \\hat \\theta_{jc} = \\frac{N_{jc}}{N_c}$$ where  $N_{jc}$ is the number of times the feature $x_j$ appears in documents of class $c$. \n\nIn this case, implementing the NBC is very simple. During fit, we only need to compute $\\hat \\pi$ and $\\hat \\theta_{jc}$. \n\n\n\n\nSee section [[Naive Bayes Classifier#Maximizing the parameters of y]] and [[Naive Bayes Classifier#Maximizing the parameters of Bernoulli features]] for detailed explanations of how to derive this result.\n\n\n#### Maximizing the parameters of y\n**TODO**\n\n#### Maximizing the parameters of Bernoulli features\n**TODO**\n\n#### Maximizing the parameters of Multinomial features \n**TODO**\n\n\n### Maximum a posteriori\n\n\nExplain The Different distribution we could use about the features and that we can merge them\n\tExplain that independently of our model, we use MLE or MAP to find the best params. Explain the creation of the model graph with and without the priors !!!!!!!!!\n\n\n\n\n\n# Resources\nhttp://www.cs.columbia.edu/~mcollins/em.pdf\nhttp://cs.brown.edu/courses/cs195-5/spring2012/lectures/2012-02-07_bayesContinuous.pdf\nhttps://cs.brown.edu/courses/csci2950-p/lectures/2013-01-29_directedGraphs.pdf\n\nhttps://colab.research.google.com/drive/17NkYm5kNPl80TVoLGpkhbs3eKYikR-lb#scrollTo=kR_TsDfsrPTq&uniqifier=1"},{"fields":{"slug":"/ML/Named Entity Recognition (NER)/","title":"Named Entity Recognition (NER)"},"frontmatter":{"draft":false},"rawBody":"NER is a subtask of information extraction that seeks to locate and classify named entities mentioned in unstructured text into pre-defined categories such as person names, organisations, locations, ... "},{"fields":{"slug":"/ML/Newton's method/","title":"Newton's Method"},"frontmatter":{"draft":false},"rawBody":"Gradient descend using second order derivative. The Taylor series is:\n$$\nf(\\boldsymbol{x}) \\approx f\\left(\\boldsymbol{x}^{(0)}\\right)+\\left(\\boldsymbol{x}-\\boldsymbol{x}^{(0)}\\right)^{\\top} \\nabla_{\\boldsymbol{x}} f\\left(\\boldsymbol{x}^{(0)}\\right)+\\frac{1}{2}\\left(\\boldsymbol{x}-\\boldsymbol{x}^{(0)}\\right)^{\\top} \\boldsymbol{H}(f)\\left(\\boldsymbol{x}^{(0)}\\right)\\left(\\boldsymbol{x}-\\boldsymbol{x}^{(0)}\\right)\n$$\n\nWe solve for the critical point of this function: \n$$\n\\boldsymbol{x}^{*}=\\boldsymbol{x}^{(0)}-\\boldsymbol{H}(f)\\left(\\boldsymbol{x}^{(0)}\\right)^{-1} \\nabla_{\\boldsymbol{x}} f\\left(\\boldsymbol{x}^{(0)}\\right)\n$$\nWhen $f$ is a positive definite quadratic function, Newton's method consists of applying the above equation to jump to the minimum, of the function directly. When the function is not quadratic but can be locally approximated as positive definite quadratic, Newton's method consists of applying the above equation multiple times."},{"fields":{"slug":"/ML/Norms/","title":"Norms"},"frontmatter":{"draft":false},"rawBody":"Norms are functions mapping vectors to non-negative values.\n\nA norm is any function $f$ that satisfies the following properties:\n$$\n\\begin{align}\n&\\bullet f(\\boldsymbol{x}) = 0 \\Rightarrow \\boldsymbol{x} = 0 \\\\\n&\\bullet f(\\boldsymbol{x} + \\boldsymbol{y}) \\le f(\\boldsymbol{x}) + f(\\boldsymbol{y}) \\\\\n&\\bullet \\forall \\alpha \\in \\mathbb{R}, f(\\alpha \\boldsymbol{x}) = |\\alpha|f(\\boldsymbol{x})\n\\end{align}\n$$\n"},{"fields":{"slug":"/ML/PCA/","title":"Maths"},"frontmatter":{"draft":false},"rawBody":"PCA is defined as an [orthogonal](https://en.wikipedia.org/wiki/Orthogonal_transformation \"Orthogonal transformation\") [linear transformation](https://en.wikipedia.org/wiki/Linear_transformation \"Linear transformation\") that transforms the data to a new [coordinate system](https://en.wikipedia.org/wiki/Coordinate_system \"Coordinate system\") such that the greatest variance by some scalar projection of the data comes to lie on the first coordinate (called the first principal component), the second greatest variance on the second coordinate, and so on.\n\n![[Pasted image 20211222145709.png | 300]]\n\n# Maths\nLet's have $X$ already mean centered (ie $X = X - \\overline{X}$). We want to find the new basis $W$ which vectors maximizes the variance along the axis. This matrix $W \\in \\mathcal{R}^{p \\times l}$ where $l \\le p$ since we can reduce the dimensionality.\nSo a sample $x_i$ is mapped to the new space as $t_i$ as: $t_{ik} = x_i \\times w_k$. \nAs the first vector $w_1$ explains most of the variance we have: \n$$\nw_i= \\underset{||w|| = 1}{argmax} ~ \\sum_{i=1}^n t_i ^ 2 = \\underset{||w|| = 1}{argmax} ~ \\sum_{i=1}^n (x_i \\times w) ^ 2\n$$\n\nUsing the matrix representation $X$: \n$$\nw_i = \\underset{||w|| = 1}{argmax} ~|| X \\times w ||^2  = \\underset{||w|| = 1}{argmax} ~\\{ w^T X^T X w \\}\n$$\nAs $w_1$ is a  unit vector, this is equivalent to:\n$$\nw_i = \\underset{w}{argmax}  ~\\{ \\frac{w^T X^T X w}{w^T w} \\}\n$$\nWe can identify here that this is problem is a Rayleigh quotient. The solution of this Equation with a Rayleigh quotient is the eigenvector with the largest eigenvalue ! \n\nTo find the $k^{th}$ vector of the $k^{th}$ largest variance, we remove the eigenvector from X:\n$$\n\\hat{X_k} = X - \\sum_{s=1}^{k-1} Xw_sw_s^T\n$$\nand then the associated eigenvalue is \n\n$$\nw_k = \\underset{||w|| =1}{argmax} ~|| \\hat{X_k} \\times w ||^2 = \\underset{w}{argmax}  ~\\{ \\frac{w^T \\hat{X_k}^T \\hat{X_k} w}{w^T w} \\}\n$$\nWhich is again a Rayleigh quotient and the solution is again an eigenvector. \n\nIn the end, the eigenvectors of $X$ are the basis that explains the variance the most."},{"fields":{"slug":"/ML/Parameter estimation/","title":"Parameter Estimation"},"frontmatter":{"draft":false},"rawBody":"We want to **estimate** (ie learn) the parameters $\\boldsymbol \\theta$  given a training set $\\mathcal{D} = \\{ (\\boldsymbol x_0, y_0), ..., (\\boldsymbol x_N, y_N) \\}$ of $N$ documents.\n\nWe can find\n- a **point estimate** of these parameters, ie a given value / vector / matrix for $\\boldsymbol \\theta$\n- or a distribution over these parameters space, ie a function $\\boldsymbol \\theta(.)$\n\nWe can find these parameters with a proper well defined training set $\\mathcal{D}$ (MLE, MAP, EM, full bayesian) or with partial data (for example we do not have labels for some features).\n\n##### MLE: Maximum Likelihood Estimate\nMLE gives a point estimate. We maximize the likelihood of the data given our model:\n\n$$\n\\hat{\\boldsymbol \\theta}_{MLE} = \\underset{\\boldsymbol \\theta}{argmax} ~ L(\\boldsymbol \\theta)\n= \\underset{\\boldsymbol \\theta}{argmax} ~ p(\\mathcal{D} | \\boldsymbol \\theta)\n$$\n\n##### MAP: Maximum a Posteriori\nMAP gives a point estimate. We maximize the likelihood of the label given the training data and defined priors:\n$$\n\\hat{\\boldsymbol \\theta}_{MAP} \n= \\underset{\\boldsymbol \\theta}{argmax} ~ p(\\boldsymbol \\theta | \\mathcal{D})\n= \\underset{\\boldsymbol \\theta}{argmax} ~ p(\\mathcal{D} | \\boldsymbol \\theta) p(\\boldsymbol \\theta)\n$$\n\n##### Bayesian estimation\nInstead of maximising the posterior distribution, we can integrate over the parameters in order to find the value of the distribution for a given variable value.\n\n\n##### Expectation maximization\n"},{"fields":{"slug":"/ML/Positive Definite Matrix/","title":"Positive Definite Matrix"},"frontmatter":{"draft":false},"rawBody":"- `semidefinite` matrix garantes: $\\forall \\boldsymbol{x}, \\boldsymbol{x}^T\\boldsymbol{A}\\boldsymbol{x} \\ge 0$\n- `definite` matrix garantes: $\\forall \\boldsymbol{x}, \\boldsymbol{x}^T\\boldsymbol{A}\\boldsymbol{x} \\ge 0$ and $\\boldsymbol{x}^T\\boldsymbol{A}\\boldsymbol{x} = 0 \\Rightarrow  \\boldsymbol{x} = 0$"},{"fields":{"slug":"/ML/RANSAC/","title":"Overview"},"frontmatter":{"draft":false},"rawBody":"**Random Sample Consensus** is an iterative method to estimate parameters of a mathematical model from a set of observed data that contains **outliers**, when outliers are to be accorded to influence on the values of the estimates. Therefore, it also can be interpreted as an outlier detection method.\n\nIt is a non-deterministic algorithm.\n\n# Overview\nThe RANSAC algorithm is a learning technique to estimate parameters of a model by random sampling of observed data. Given a dataset whose data elements contain both inliers and outliers, RANSAC uses the voting scheme to find the optimal fitting result. Data elements in the dataset are used to vote for one or multiple models. The implementation of this voting scheme is based on two assumptions: that the noisy features will not vote consistently for any single model (few outliers) and there are enough features to agree on a good model (few missing data). The RANSAC algorithm is essentially composed of two steps that are iteratively repeated:\n\n1.  In the first step, a sample subset containing minimal data items is randomly selected from the input dataset. A fitting model and the corresponding model parameters are computed using only the elements of this sample subset. The cardinality of the sample subset is the smallest sufficient to determine the model parameters.\n2.  In the second step, the algorithm checks which elements of the entire dataset are consistent with the model instantiated by the estimated model parameters obtained from the first step. A data element will be considered as an outlier if it does not fit the fitting model instantiated by the set of estimated model parameters within some error threshold that defines the maximum deviation attributable to the effect of noise.\n\nThe set of inliers obtained for the fitting model is called the consensus set. The RANSAC algorithm will iteratively repeat the above two steps until the obtained consensus set in certain iteration has enough inliers.\n\nThe input to the RANSAC algorithm is a set of observed data values, a way of fitting some kind of model to the observations, and some [confidence](https://en.wikipedia.org/wiki/Confidence_interval \"Confidence interval\") parameters. RANSAC achieves its goal by repeating the following steps:\n\n1.  Select a random subset of the original data. Call this subset the _hypothetical inliers_.\n2.  A model is fitted to the set of hypothetical inliers.\n3.  All other data are then tested against the fitted model. Those points that fit the estimated model well, according to some model-specific [loss function](https://en.wikipedia.org/wiki/Loss_function \"Loss function\"), are considered as part of the _consensus set_.\n4.  The estimated model is reasonably good if sufficiently many points have been classified as part of the consensus set.\n5.  Afterwards, the model may be improved by reestimating it using all members of the consensus set.\n\nThis procedure is repeated a fixed number of times, each time producing either a model which is rejected because too few points are part of the consensus set, or a refined model together with a corresponding consensus set size. In the latter case, we keep the refined model if its consensus set is larger than the previously saved model.\n\n![[Pasted image 20211231124420.png]]"},{"fields":{"slug":"/ML/Regression losses/","title":"Regression Losses"},"frontmatter":{"draft":false},"rawBody":"| Name           | Equation          | Comments                                      |\n|----------------|-------------------|-----------------------------------------------|\n| MSE            | $\\frac{1}{m} \\sum_i \\left(\\hat{\\boldsymbol{y}}-\\boldsymbol{y}\\right)_{i}^{2}$ |                                      |\n| Name           | Equation          |                                       |\n| Name           | Equation          |                                       |"},{"fields":{"slug":"/ML/Singular Value Decomposition (SVD)/","title":"Singular Value Decomposition (SVD)"},"frontmatter":{"draft":false},"rawBody":"Similar to [[Eigendecomposition]] but decomposes a matrix into `singular vectors` and `singular values`. SVD is more general than Eigendecomposition because all real matrix have an SVD.\n\nSVD decomposes $\\boldsymbol{A}$ as:\n$$ \n\\begin{align}\n&\\boldsymbol{A} =  \\boldsymbol{U}\\boldsymbol{D}\\boldsymbol{V}^{T} \\\\\n&\\text{ where } \n\\boldsymbol{A} \\in \\mathbb{R}^{m \\times n}, \n\\boldsymbol{U} \\in \\mathbb{R}^{m \\times m}, \n\\boldsymbol{D} \\in \\mathbb{R}^{m \\times n}, \n\\boldsymbol{V} \\in \\mathbb{R}^{n \\times n}\n\\end{align}\n$$\n$\\boldsymbol{U}$ and $\\boldsymbol{V}$ are both `orthogonal` matrices. $\\boldsymbol{D}$ is defined to be a diagonal matrix, but is not necessarily square.\n\nWe can use SVD to generalise matrix inversion to non-sqaure matrices, see [[Moore-Penrose Pseudoinverse]].\n"},{"fields":{"slug":"/ML/Symmetric matrix/","title":"Symmetric Matrix"},"frontmatter":{"draft":false},"rawBody":"$\\boldsymbol{A} = \\boldsymbol{A}^T$"},{"fields":{"slug":"/ML/Thompson Sampling/","title":"Thompson Sampling"},"frontmatter":{"draft":false},"rawBody":"Used for Multi-armed bandit. It is a heuristic of which arm to choose from to balance exploration vs exploitation.\nIn practice, the rule is implemented by sampling. In each round, parameters $\\theta^*$ are sampled from the posterior $P(\\theta |\\mathcal  {D})$, and an action $a^*$  chosen that maximizes $\\mathbb{E} [r | \\theta^*, a^*, x]$, i.e. the expected reward given the sampled parameters, the action, and the current context. Conceptually, this means that the player instantiates their beliefs randomly in each round according to the posterior distribution, and then acts optimally according to them.\n\n![[Pasted image 20220413210905.png]]"},{"fields":{"slug":"/ML/Trace of matrix/","title":"Trace of Matrix"},"frontmatter":{"draft":false},"rawBody":"- $Tr(\\boldsymbol{A}) = \\sum_i \\boldsymbol{A}_{i,i}$  \n- $Tr(\\boldsymbol{A}) = Tr(\\boldsymbol{A}^T)$    \n- $Tr(\\boldsymbol{A}\\boldsymbol{B}\\boldsymbol{C}) = Tr(\\boldsymbol{C}\\boldsymbol{A}\\boldsymbol{B}) = Tr(\\boldsymbol{B}\\boldsymbol{C}\\boldsymbol{A})$\n- "},{"fields":{"slug":"/ML/Variance and Standard error/","title":"Variance and Standard Error"},"frontmatter":{"draft":false},"rawBody":"\n|Name | Formula  | Comment  |\n|----------------------------------|----------------------------------|-------------------------------|\n|Variance of population            |$\\frac{1}{n} \\sum (x_i - \\mu)^2$  |   \t\t\t\t\t\t       |\n|Variance of **sample**     \t   |$\\frac{1}{n-1}\\sum(x_i-\\mu)^2$    | **<-  notice the N - 1 !!!**  |\n|Standard error of the mean \t   |$\\frac{\\sigma}{\\sqrt(n)}$         |                               |\n|Confidence interval for a var \t   |$P(L<X<U)$ gives $L=\\mu-z\\sigma, U=\\mu+z\\sigma$ | [see standard score](https://en.wikipedia.org/wiki/Standard_score) |\n|Confidence interval for a Mean !! |$P(L<X<U)$ gives $L=\\mu-z\\frac{\\sigma}{\\sqrt(n)}, U=\\mu+z\\frac{\\sigma}{\\sqrt(n)}$ | [see standard score](https://en.wikipedia.org/wiki/Standard_score) |\n"},{"fields":{"slug":"/ML/trust region/","title":"Trust Region"},"frontmatter":{"draft":false},"rawBody":"[Trust Region](https://en.wikipedia.org/wiki/Trust_region)"},{"fields":{"slug":"/Papers/VICREG  variance-invariance-covariance regulaization for self-supervised Learning/","title":"Questions"},"frontmatter":{"draft":false},"rawBody":"# Questions\n### Do we need both network for inference / Which network do we chose ? \n### What is collapse ? Are there different way of collapsing ? \n- All embedding vectors collapse to a trivbial constant solution \n- Dimentional collapse: the embedding vectors end up spanning a lower-dimensional subspace instead of the entire available embedding space. See [UNDERSTANDINGDIMENSIONALCOLLAPSE INCON-TRASTIVESELF-SUPERVISEDLEARNING](https://arxiv.org/pdf/2110.09348.pdf)\n- \n### Why do we need both Covariance and Variance loss terms ? \n### What is the benefit of having different archictectures and different weights ? \n\n### Why decorelating at embeddings lvl also decorrelate at representation lvl ? \n\n# Minor questions\n### What is LARS optimizer ? \n\n"},{"fields":{"slug":"/Programming/Dynamic Programming/","title":"A- What is Dynamic Programming?"},"frontmatter":{"draft":false},"rawBody":"From: https://leetcode.com/explore/featured/card/dynamic-programming/630/an-introduction-to-dynamic-programming/4034/\n# A- What is Dynamic Programming?\n**Dynamic Programming** (DP) is a programming paradigm that can systematically and efficiently explore all possible solutions to a problem. As such, it is capable of solving a wide variety of problems that often have the following characteristics:\n\n1.  The problem can be broken down into \"overlapping subproblems\" - smaller versions of the original problem that are re-used multiple times.\n2.  The problem has an \"optimal substructure\" - an optimal solution can be formed from optimal solutions to the overlapping subproblems of the original problem.\n\nAs a beginner, these theoretical definitions may be hard to wrap your head around. Don't worry though - at the end of this chapter, we'll talk about how to practically spot when DP is applicable. For now, let's look a little deeper at both characteristics.\n\nThe [Fibonacci sequence](https://en.wikipedia.org/wiki/Fibonacci_number) is a classic example used to explain DP. For those who are unfamiliar with the Fibonacci sequence, it is a sequence of numbers that starts with 0, 1, and each subsequent number is obtained by adding the previous two numbers together.\n\nIf you wanted to find the $n^{th}$ Fibonacci number $F(n)$, you can break it down into smaller **subproblems** - find $F(n - 1)$ and $F(n - 2)$ instead. Then, adding the solutions to these subproblems together gives the answer to the original question, $F(n - 1) + F(n - 2) = F(n)$, which means the problem has **optimal substructure**, since a solution $F(n)$ to the original problem can be formed from the solutions to the subproblems. These subproblems are also **overlapping** - for example, we would need $F(4)$ to calculate both $F(5)$ and $F(6)$.\n\nThese attributes may seem familiar to you. Greedy problems have optimal substructure, but not overlapping subproblems. Divide and conquer algorithms break a problem into subproblems, but these subproblems are not **overlapping** (which is why DP and divide and conquer are commonly mistaken for one another).\n\nDynamic programming is a powerful tool because it can break a complex problem into manageable subproblems, avoid unnecessary recalculation of overlapping subproblems, and use the results of those subproblems to solve the initial complex problem. DP not only aids us in solving complex problems, but it also greatly improves the time complexity compared to brute force solutions. For example, the brute force solution for calculating the Fibonacci sequence has exponential time complexity, while the dynamic programming solution will have linear time complexity. Throughout this explore card, you will gain a better understanding of what makes DP so powerful. In the next section, we'll discuss the two main methods of implementing a DP algorithm.\n\n# Top-down and Bottom-up\n\nThere are two ways to implement a DP algorithm:\n\n1.  Bottom-up, also known as tabulation.\n2.  Top-down, also known as memoization.\n\nLet's take a quick look at each method.\n\n## Bottom-up (Tabulation)\n\nBottom-up is implemented with iteration and starts at the base cases. Let's use the Fibonacci sequence as an example again. The base cases for the Fibonacci sequence are $F(0) = 0$ and$F(1) = 1$. With bottom-up, we would use these base cases to calculate $F(2)$, and then use that result to calculate $F(3)$, and so on all the way up to $F(n)$.\n\n```\n// Pseudocode example for bottom-up\n\nF = array of length (n + 1)\nF[0] = 0\nF[1] = 1\nfor i from 2 to n:\n    F[i] = F[i - 1] + F[i - 2]\n```\n\n## Top-down (Memoization)\nTop-down is implemented with recursion and made efficient with memoization. If we wanted to find the $n^{th}$ Fibonacci number F(n)F(n), we try to compute this by finding $F(n - 1)$ and $F(n - 2)$. This defines a recursive pattern that will continue on until we reach the base cases $F(0) = F(1) = 1$. The problem with just implementing it recursively is that there is a ton of unnecessary repeated computation. Take a look at the recursion tree if we were to find $F(5)$:\n\n![[Pasted image 20220206215709.png | 500]]\nNotice that we need to calculate $F(2)$ three times. This might not seem like a big deal, but if we were to calculate $F(6)$, this **entire image** would be only one child of the root. Imagine if we wanted to find $F(100)$ - the amount of computation is exponential and will quickly explode. The solution to this is to **memoize** results.\n\n> **memoizing** a result means to store the result of a function call, usually in a hashmap or an array, so that when the same function call is made again, we can simply return the **memoized** result instead of recalculating the result.\n\nAfter we calculate $F(2)$, let's store it somewhere (typically in a hashmap), so in the future, whenever we need to find $F(2)$, we can just refer to the value we already calculated instead of having to go through the entire tree again. Below is an example of what the recursion tree for finding $F(6)$ looks like with and without memoization.\n\n```\n// Pseudocode example for top-down\n\nmemo = hashmap\nFunction F(integer i):\n    if i is 0 or 1: \n        return i\n    if i doesn't exist in memo:\n        memo[i] = F(i - 1) + F(i - 2)\n    return memo[i]\n```\n\n### Which is better?\n\nAny DP algorithm can be implemented with either method, and there are reasons for choosing either over the other. However, each method has one main advantage that stands out:\n\n-   A bottom-up implementation's runtime is usually faster, as iteration does not have the overhead that recursion does.\n-   A top-down implementation is usually much easier to write. This is because with recursion, the ordering of subproblems does not matter, whereas with tabulation, we need to go through a logical ordering of solving subproblems.\n\n> We'll be talking more about these two options throughout the card. For now, all you need to know is that top-down uses recursion, and bottom-up uses iteration.\n\n\n# When to Use DP\n\nWhen it comes to solving an algorithm problem, especially in a high-pressure scenario such as an interview, half the battle is figuring out how to even approach the problem. In the first section, we defined what makes a problem a good candidate for dynamic programming. Recall:\n\n1.  The problem can be broken down into \"overlapping subproblems\" - smaller versions of the original problem that are re-used multiple times\n2.  The problem has an \"optimal substructure\" - an optimal solution can be formed from optimal solutions to the overlapping subproblems of the original problem\n\nUnfortunately, it is hard to identify when a problem fits into these definitions. Instead, let's discuss some common characteristics of DP problems that are easy to identify.\n\n**The first characteristic** that is common in DP problems is that the problem will ask for the optimum value (maximum or minimum) of something, or the number of ways there are to do something. For example:\n\n-   What is the minimum cost of doing...\n-   What is the maximum profit from...\n-   How many ways are there to do...\n-   What is the longest possible...\n-   Is it possible to reach a certain point...\n\n> **Note:** Not all DP problems follow this format, and not all problems that follow these formats should be solved using DP. However, these formats are very common for DP problems and are generally a hint that you should consider using dynamic programming.\n\nWhen it comes to identifying if a problem should be solved with DP, this first characteristic is not sufficient. Sometimes, a problem in this format (asking for the max/min/longest etc.) is meant to be solved with a greedy algorithm. The next characteristic will help us determine whether a problem should be solved using a greedy algorithm or dynamic programming.\n\n**The second characteristic** that is common in DP problems is that future \"decisions\" depend on earlier decisions. Deciding to do something at one step may affect the ability to do something in a later step. This characteristic is what makes a greedy algorithm invalid for a DP problem - we need to factor in results from previous decisions. Admittedly, this characteristic is not as well defined as the first one, and the best way to identify it is to go through some examples.\n\n[House Robber](https://leetcode.com/problems/house-robber/) is an excellent example of a dynamic programming problem. The problem description is:\n\n> You are a professional robber planning to rob houses along a street. Each house has a certain amount of money stashed, the only constraint stopping you from robbing each of them is that adjacent houses have security systems connected and it will automatically contact the police if two adjacent houses were broken into on the same night.  \n>   \n> Given an integer array nums representing the amount of money of each house, return the maximum amount of money you can rob tonight without alerting the police.\n\nIn this problem, each decision will affect what options are available to the robber in the future. For example, with the test case \\text{nums = [2, 7, 9, 3, 1]}nums = [2, 7, 9, 3, 1], the optimal solution is to rob the houses with \\text{2}2, \\text{9}9, and \\text{1}1 money. However, if we were to iterate from left to right in a greedy manner, our first decision would be whether to rob the first or second house. 7 is way more money than 2, so if we were greedy, we would choose to rob house 7. However, this prevents us from robbing the house with 9 money. As you can see, our decision between robbing the first or second house affects which options are available for future decisions.\n\n[Longest Increasing Subsequence](https://leetcode.com/problems/longest-increasing-subsequence/) is another example of a classic dynamic programming problem. In this problem, we need to determine the length of the longest (first characteristic) subsequence that is strictly increasing. For example, if we had the input \\text{nums = [1, 2, 6, 3, 5]}nums = [1, 2, 6, 3, 5], the answer would be 4, from the subsequence \\text{[1, 2, 3, 5]}[1, 2, 3, 5]. Again, the important decision comes when we arrive at the 6 - do we take it or not take it? If we decide to take it, then we get to increase our current length by 1, but it affects the future - we can no longer take the 3 or 5. Of course, with such a small example, it's easy to see why we shouldn't take it - but how are we supposed to design an algorithm that can always make the correct decision with huge inputs? Imagine if nums contained 10,00010,000 numbers instead.\n\n> When you're solving a problem on your own and trying to decide if the second characteristic is applicable, assume it isn't, then try to think of a counterexample that proves a greedy algorithm won't work. If you can think of an example where earlier decisions affect future decisions, then DP is applicable.\n\nTo summarize: if a problem is asking for the maximum/minimum/longest/shortest of something, the number of ways to do something, or if it is possible to reach a certain point, it is probably greedy or DP. With time and practice, it will become easier to identify which is the better approach for a given problem. Although, in general, if the problem has constraints that cause decisions to affect other decisions, such as using one element prevents the usage of other elements, then we should consider using dynamic programming to solve the problem. **These two characteristics can be used to identify if a problem should be solved with DP.**\n\n> Note: these characteristics should only be used as guidelines - while they are extremely common in DP problems, at the end of the day DP is a very broad topic.\n\n\n# Framework for DP Problems\n\nNow that we understand the basics of DP and how to spot when DP is applicable to a problem, we've reached the most important part: actually solving the problem. In this section, we're going to talk about a framework for solving DP problems. This framework is applicable to nearly every DP problem and provides a clear step-by-step approach to developing DP algorithms.\n\n> For this article's explanation, we're going to use the problem [Climbing Stairs](https://leetcode.com/problems/climbing-stairs/) as an example, with a top-down (recursive) implementation. Take a moment to read the problem description and understand what the problem is asking.\n\nBefore we start, we need to first define a term: **state**. In a DP problem, a **state** is a set of variables that can sufficiently describe a scenario. These variables are called **state variables**, and we only care about relevant ones. For example, to describe every scenario in Climbing Stairs, there is only 1 relevant state variable, the current step we are on. We can denote this with an integer \\text{i}i. If \\text{i = 6}i = 6, that means that we are describing the state of being on the 6th step. Every unique value of \\text{i}i represents a unique **state**.\n\n> You might be wondering what \"relevant\" means here. Picture this problem in real life: you are on a set of stairs, and you want to know how many ways there are to climb to say, the 10th step. We're definitely interested in what step you're currently standing on. However, we aren't interested in what color your socks are. You could certainly include sock color as a state variable. Standing on the 8th step wearing green socks is a different state than standing on the 8th step wearing red socks. However, changing the color of your socks will not change the number of ways to reach the 10th step from your current position. Thus the color of your socks is an **irrelevant** variable. In terms of figuring out how many ways there are to climb the set of stairs, the only **relevant** variable is what stair you are currently on.\n\n  \n\n### The Framework\n\nTo solve a DP problem, we need to combine 3 things:\n\n1.  **A function or data structure that will compute/contain the answer to the problem for every given state**.\n    \n    For Climbing Stairs, let's say we have an function \\text{dp}dp where \\text{dp(i)}dp(i) returns the number of ways to climb to the i^{th}ith step. Solving the original problem would be as easy as \\text{return dp(n)}return dp(n).\n    \n    How did we decide on the design of the function? The problem is asking \"How many distinct ways can you climb to the top?\", so we decide that the function will represent how many distinct ways you can climb to a certain step - literally the original problem, but generalized for a given state.\n    \n    > Typically, top-down is implemented with a recursive function and hash map, whereas bottom-up is implemented with nested for loops and an array. When designing this function or array, we also need to decide on state variables to pass as arguments. This problem is very simple, so all we need to describe a state is to know what step we are currently on \\text{i}i. We'll see later that other problems have more complex states.\n    \n2.  **A recurrence relation to transition between states.**\n    \n    A recurrence relation is an equation that relates different states with each other. Let's say that we needed to find how many ways we can climb to the 30th stair. Well, the problem states that we are allowed to take either 1 or 2 steps at a time. Logically, that means to climb to the 30th stair, we arrived from either the 28th or 29th stair. Therefore, the number of ways we can climb to the 30th stair is equal to the number of ways we can climb to the 28th stair plus the number of ways we can climb to the 29th stair.  \n      \n    The problem is, we don't know how many ways there are to climb to the 28th or 29th stair. However, we can use the logic from above to define a recurrence relation. In this case, \\text{dp(i) = dp(i - 1) + dp(i - 2)}dp(i) = dp(i - 1) + dp(i - 2). As you can see, information about some states gives us information about other states.\n    \n    > Upon careful inspection, we can see that this problem is actually the Fibonacci sequence in disguise! This is a very simple recurrence relation - typically, finding the recurrence relation is the most difficult part of solving a DP problem. We'll see later how some recurrence relations are much more complicated, and talk through how to derive them.\n    \n3.  **Base cases, so that our recurrence relation doesn't go on infinitely.**\n    \n    The equation \\text{dp(i) = dp(i - 1) + dp(i - 2)}dp(i) = dp(i - 1) + dp(i - 2) on its known will continue forever to negative infinity. We need base cases so that the function will eventually return an actual number.  \n      \n    Finding the base cases is often the easiest part of solving a DP problem, and just involves a little bit of logical thinking. When coming up with the base case(s) ask yourself: What state(s) can I find the answer to without using dynamic programming? In this example, we can reason that there is only 1 way to climb to the first stair (1 step once), and there are 2 ways to climb to the second stair (1 step twice and 2 steps once). Therefore, our base cases are \\text{dp(1) = 1}dp(1) = 1 and \\text{dp(2) = 2}dp(2) = 2.\n    \n    > We said above that we don't know how many ways there are to climb to the 28th and 29th stairs. However, using these base cases and the recurrence relation from step 2, we can figure out how many ways there are to climb to the 3rd stair. With that information, we can find out how many ways there are to climb to the 4th stair, and so on. Eventually, we will know how many ways there are to climb to the 28th and 29th stairs.\n    \n\n  \n\n### Example Implementations\n\nHere is a basic top-down implementation using the 3 components from the framework:\n```python\nclass Solution:\n    def climbStairs(self, n: int) -> int:\n        def dp(i): \n            \"\"\"A function that returns the answer to the problem for a given state.\"\"\"\n            # Base cases: when i is less than 3 there are i ways to reach the ith stair.\n            if i <= 2: \n                return i\n            \n            # If i is not a base case, then use the recurrence relation.\n            return dp(i - 1) + dp(i - 2)\n        \n        return dp(n)\n```\nDo you notice something missing from the code? We haven't memoized anything! The code above has a time complexity of O(2^n)O(2n) because every call to \\text{dp}dp creates 2 more calls to \\text{dp}dp. If we wanted to find how many ways there are to climb to the 250th step, the number of operations we would have to do is approximately equal to the number of atoms in the universe.\n\nIn fact, without the memoization, this isn't actually dynamic programming - it's just basic recursion. Only after we optimize our solution by adding memoization to avoid repeated computations can it be called DP. As explained in chapter 1, memoization means caching results from function calls and then referring to those results in the future instead of recalculating them. This is usually done with a hashmap or an array.\n\n```python\nclass Solution:\n    def climbStairs(self, n: int) -> int:\n        def dp(i):\n            if i <= 2: \n                return i\n            if i not in memo:\n                # Instead of just returning dp(i - 1) + dp(i - 2), calculate it once and then\n                # store the result inside a hashmap to refer to in the future.\n                memo[i] = dp(i - 1) + dp(i - 2)\n            \n            return memo[i]\n        \n        memo = {}\n        return dp(n)\n```\nWith memoization, our time complexity drops to O(n)O(n) - astronomically better, literally.\n\n> You may notice that a hashmap is overkill for caching here, and an array can be used instead. This is true, but using a hashmap isn't necessarily bad practice as some DP problems will require one, and they're hassle-free to use as you don't need to worry about sizing an array correctly. Furthermore, when using top-down DP, some problems do not require us to solve every single subproblem, in which case an array may use more memory than a hashmap.\n\nWe just talked a whole lot about top-down, but what about bottom-up? Everything is pretty much the same, except we will start from our base cases and iterate up to our final answer. As stated before, bottom-up implementations usually use an array, so we will use an array \\text{dp}dp where \\text{dp[i]}dp[i] represents the number of ways to climb to the i^{th}ith step.\n\n```python\nclass Solution:\n    def climbStairs(self, n: int) -> int:\n        if n == 1:\n            return 1\n            \n        # An array that represents the answer to the problem for a given state\n        dp = [0] * (n + 1)\n        dp[1] = 1 # Base cases\n        dp[2] = 2 # Base cases\n        \n        for i in range(3, n + 1):\n            dp[i] = dp[i - 1] + dp[i - 2] # Recurrence relation\n\n        return dp[n]\n```\n\n> Notice that the implementation still follows the framework exactly - the framework holds for both top-down and bottom-up implementations.\n\n  \n\n### To Summarize\n\nWith DP problems, we can use logical thinking to find the answer to the original problem for certain inputs, in this case we reason that there is 1 way to climb to the first stair and 2 ways to climb to the second stair. We can then use a recurrence relation to find the answer to the original problem for any state, in this case for any stair number. Finding the recurrence relation involves thinking about how moving from one state to another changes the answer to the problem.\n\nThis is the essence of dynamic programming. Here's a quick animation for Climbing Stairs:\n\n\n# Time and Space Complexity\n\nFinding the time and space complexity of a dynamic programming algorithm may sound like a daunting task. However, this task is usually not as difficult as it sounds. Furthermore, justifying the time and space complexity in an explanation is relatively simple as well. One of the main points with DP is that we never repeat calculations, whether by tabulation or memoization, we only compute a state once. Because of this, the time complexity of a DP algorithm is directly tied to the number of possible states.\n\nIf computing each state requires FF time, and there are nn possible states, then the time complexity of a DP algorithm is O(n \\cdot F)O(n⋅F). With all the problems we have looked at so far, computing a state has just been using a recurrence relation equation, which is O(1)O(1). Therefore, the time complexity has just been equal to the number of states. To find the number of states, look at each of your state variables, compute the number of values each one can represent, and then multiply all these numbers together.\n\nLet's say we had 3 state variables: \\text{i}i, \\text{k}k, and \\text{holding}holding for some made up problem. \\text{i}i is an integer used to keep track of an index for an input array \\text{nums}nums, \\text{k}k is an integer given in the input which represents the maximum actions we can do, and \\text{holding}holding is a boolean variable. What will the time complexity be for a DP algorithm that solves this problem? Let \\text{n = nums.length}n = nums.length and \\text{K}K be the maximum actions possible given in the input. \\text{i}i can be from \\text{0}0 to \\text{nums.length}nums.length, \\text{k}k can be from \\text{0}0 to \\text{K}K, and \\text{holding}holding }can be true or false. Therefore, there are \\text{n} \\cdot \\text{K} \\cdot \\text{2}n⋅K⋅2 states. If computing each state is O(1)O(1), then the time complexity will be O(n \\cdot K \\cdot 2) = O(n \\cdot K)O(n⋅K⋅2)=O(n⋅K).\n\nWhenever we compute a state, we also store it so that we can refer to it in the future. In bottom-up, we tabulate the results, and in top-down, states are memoized. Since we store states, the space complexity is equal to the number of states. That means that in problems where calculating a state is O(1)O(1), the time and space complexity are the same. In many DP problems, there are optimizations that can improve both complexities - we'll talk about this later."},{"fields":{"slug":"/Programming/Peak and Valleys/","title":"Example problem: Min Rewards (hard pb)"},"frontmatter":{"draft":false},"rawBody":"AKA min and max\n# Example problem: Min Rewards (hard pb)\nImagine that you're a teacher who's just graded the final exam in a class. You have a list of student scores on the final exam in a particular order (not necessarily sorted), and you want to reward your students. You decide to do so fairly by giving them arbitrary rewards following two rules:\n1. All students must receive at least one reward.\n2. Any given student must receive strictly more rewards than an adjacent student (a student immediately to the left or to the right) with a lower score and must receive strictly fewer rewards than an adjacent student with a higher score.\n\nWrite a function that takes in a list of scores and returns the minimum number of rewards that you must give out to students to satisfy the two rules.\nYou can assume that all students have different scores; in other words, the scores are all unique.\n\n### Samples input: \n```python\nscores = [8, 4, 2, 1, 3, 6, 7, 9, 5]\n```\n\n### Sample output:\n```python\n25 // you would give out the following rewards: [4, 3, 2, 1, 2, 3, 4, 5, 1]\n```\n\n# Solution Peak and Valley\n## Version 1\nIdentify the minimums in the list of scores. Each values that are local minimum (smaller than the value to the left or right) are going to be assigned a `1`. \nThen we want to go to the left and right of these local minimums, incrementing the rewards for each values.\n\n```\n### to lazy\n```\n\n## Version 2 (optimal)\nOnce we understood the previous solution, we can realise that we do not need to start from the local minimums. We can iterate from left to right, incrementing a value if the current value is greater than the previous. Then iterating in reverse order, and taking the max of current rewards or next rewards + 1 if the current value is greater than the next one.\n\n```python\n# O(n) time | O(n) space \ndef minRewards(scores):\n    rewards = [1] * len(scores)\n    for i in range(1, len(scores)):\n\t\tif scores[i] > scores[i - 1]:\n\t\t\trewards[i] = rewards[i - 1] + 1\n\tfor i in reversed(range(len(scores) - 1)):\n\t\tif scores[i] > scores[i + 1]:\n\t\t\trewards[i] = max(rewards[i], rewards[i + 1] + 1)\n\treturn sum(rewards)\n```\n"},{"fields":{"slug":"/Python/Dictionary/","title":"Dictionary"},"frontmatter":{"draft":false},"rawBody":"See Blob post about CPython implementation of Dict: https://www.laurentluce.com/posts/python-dictionary-implementation/  \nAnd original docstring comment in CPython code: https://github.com/python/cpython/blob/main/Objects/dictobject.c\n\n```python\nclass Dict:\n\n\tdef __init__(self, size=100):\n\t\tself.storage = [list() for _ in range(size)]\n\t\tself.size = size\n\t\tself.length = 0\n\t\n\tdef __setitem__(self, key, value):\n\t\tkey_pos = self._get_pos(key)\n\t\tfor pairs in self.storage[key_pos]:\n\t\t\tif key == pairs[0]:\n\t\t\t\t# update existing key with new value\n\t\t\t\tpairs[1] = value\n\t\t\t\tbreak\n\t\telse:\n\t\t\tself.storage[key_pos].append([key, value])\n\t\t\tself.length += 1\n\n\tdef __getitem__(self, key):\n\t\tkey_pos = self._get_pos(key)\n\t\t\tfor pairs in self.storage[key_pos]:\n\t\t\t\tif key == pairs[0]:\n\t\t\t\t\treturn pairs[1]\n\t\traise KeyError(f'Key {key} does not exists.')\n\t\n\t  \n\t\n\tdef __delitem__(self, key):\n\t\tkey_pos = self._get_pos(key)\n\t\tidx_to_remove = None\n\t\tfor idx, pairs in enumerate(self.storage[key_pos]):\n\t\t\tif key == pairs[0]:\n\t\t\t\tidx_to_remove = idx\n\t\t\t\tbreak\n\t\telse:\n\t\t\traise KeyError()\n\n\t\tdel self.storage[key_pos][idx_to_remove]\t\t\n\t\tself.length -=1\n\t\n\t  \n\t\n\tdef __len__(self):\n\t\treturn self.length\n\t\n\tdef __contains__(self, key):\n\t\tkey_pos = self._get_pos(key)\n\t\tfor pairs in self.storage[key_pos]:\n\t\t\tif key == pairs:\n\t\t\t\treturn True\n\t\treturn False\n\t\t\n\tdef __iter__(self):\n\t\tfor k, v in self.items():\n\t\t\tyield k\n\t\n\tdef keys(self):\n\t\treturn self.__iter__()\n\t\n\tdef values(self):\n\t\tfor k, v in self.items():\n\t\t\tyield v\n\t\n\tdef items(self):\n\t\tfor pairs in self.storage:\n\t\t\tfor pair in pairs:\n\t\t\t\tyield pair\n\n\tdef get(self, key, default=None):\n\t\ttry:\n\t\t\treturn self[key]\n\t\texcept KeyError:\n\t\t\treturn default\n\t\n\tdef _get_pos(self, key):\n\t\treturn hash(key) % self.size\n```"},{"fields":{"slug":"/Python/GIL/","title":"References"},"frontmatter":{"draft":false},"rawBody":"> [!tldr] Global Interpreter Lock\n> The GIL is a mutex (or a lock) that allows only one thread to hold the control of the Python interpreter.\n> \n> - Use multi-threading for I/O bound programs because the thread has to wait to get the data.\n> - Do NOT use multi-threading for CPU bound operations. They will run more slowly than in a single thread. Consider *multi-processing* instead, which runs multiple python instances.  \n\nThe root of the cause: to solve memory management automatically, Python uses **reference counting** (which is different to Garbage Collection). \nThe reference count needs to be protected against **race conditions**. The reference count variable is kept safe by adding **locks** to all data structures that are shared across threads so that they are not modified inconsistently.\nThe GIL is a single lock on the interpreter itself, which saves having many locks, but enforces python to be executed by one thread at a time.\n\n\n\n# References\nhttps://realpython.com/python-gil/\nhttps://www.youtube.com/watch?v=Obt-vMVdM8s"},{"fields":{"slug":"/Python/Is Python Compiled or Interpreted language/","title":"Is Python Compiled or Interpreted Language"},"frontmatter":{"draft":false},"rawBody":"Great answer here: https://discuss.python.org/t/is-python-a-compiled-language-or-an-interpreted-language/6556\n"},{"fields":{"slug":"/Courses/AlgoExpert/Algo Expert - Behavioral interview/","title":"General tips"},"frontmatter":{"draft":false},"rawBody":"**Behavioural interviews** aim to evaluate skills that enables the worker to perform well. These are skills that come about with maturity, working experience, mistakes, successes and so on. \n\nThis interview is mainly a one sided conversation. Most questions will be about your passed experiences. With some other questions like *\"Why do you like this company\"*. \n\nDepending on the company, the job offer and the candidate, the behavioural interview will be quite different and the bar to pass will vary greatly.\n\nExamples:\n- People skill\n\t- How do you motivate people you manage\n\t- How do you resolve conflicts\n- Experienced / seasoned worker:\n\t- How do you handle problems in production\n\t- How do you deal with stress \n\t- Present to failures (show you learn from mistakes)\n\t- Present some success (show some humility)\n- Structured:\n\t- How do you approach new problem\n\t- How do you distribute tasks\n- Handle working in a company with different stake holders:\n\t- How do you present to non-technical ppl ideas\n\t- How do you convince people\n\n\n\n# General tips\n**You have to prepare for Behavioural interviews.** If you think you can go without preparation, think again.\n\n1. Never reply in a manner that is:\n\t- offensive, disrespectful, confrontational or arrogant.\n\t- **too vague**: The interviewers are looking for signals to assess your skill. So you have to be specific, how you reacted to events, what you learned from these events, what you actually contributed to.\n\n\n2. Positive points you want to include:\n\t- Strike a balance between good qualities and ability to work with others. \n\t- Try to answer question with stories that highlight the above point.\n\n3. Do not lie.\n4. Make sure to talk only about the relevant stuff. \n5. *\"What happened when you had a low performer?\"* But what if you never had this scenario ? Mention it to the interviewer, and say that you can say how you would react if it were to happen.\n6. General comm skills. You don't need to be a great speaker, but you want to be concise, specific, use the right answer for the right question.\n7. To best prepare, do mock interviews. See how you would answer and how other people answer them.\n\n\n# Example questions\n## Imagine you have a low performer in your team. How would you handle this ? \n\nYou need to ask more question or give context in your answer: \n- in what way they were a low performer. Did they introduce one bug ? Multiple bugs ? In production ? \n- Is the low performance their fault or not their fault ?\n- Is the code base already riddled with tech debt ? \n\n\nHow to help them ? \n- Provide constructive feedback. \n- Offer help, unblock them, let them know they can ask for help.\n- Take notes on ALL the steps that were taken to help the person. This is useful because it gives context, and could help the poor performer next time (as in the issue was not them so they should not be blamed for it, or they improved since then.) But also if no improvement is coming then you can escalate above.\n\n## Team conflict: describe a time when there was a conflict within the team ? Describe how you handle it and how you mitigate them in the future ? \n#### My Story:\n- Callsign: Peter writing bad, convoluted code. Was not receptive for improvements\n- BP: \n\t- Mono repo vs Multi repo ? \n\t- BPCS ? \n\t- Order of features to work on with Rusen for BPCS POC delivery ? \n\t- Conflicts with John ????????????????\n\n#### How I would resolve a theoretical conflict ? \nMy learnings so far\n- assume good intent. if someone strongly reacts, it is because they have good intent. \n- Is it really that important to be in conflict over such an issue ? \n\n\n# Why do you want to work at \\<company name\\> ? \nI want to work at Meta for 3 reasons: \n\n1. technologies that really interests me\n2. Meta is delivery driven. So I would love to be able to work in a team that has higher velocities.\n3. The bootcamp experience. Being able to check first the teams and see later which one you prefer.\n\n\n# Describe a time when you strongly disagreed with a co-worker about an engineering decision. How did you go about making the final decision and what happened after ?\n#### My Story: \nMy learnings so far\n- I would make sure we are assuming the same problems, talking about the same issues. It happened in the past that we were talking about slightly different problems so we would not understand each other.\n- The more specific you get, the less ambiguity there is. Often there are no problems when you dig into the details. It is generalisation that blurs the lines and make some ideas collide.\n- Present the non technical arguments. Maybe the feature is required, maybe we have some other constraints like time.\n\n#### My Example: \nIntroducing a test framework in my team. \nThe research team was more relaxed about testing due to lack of production code at the time. I presented my ideas about testing framework that would enable:\n- Code sharing\n- Ensure reliable code base in the future.\n\nThis has proven to be a good decision. A lot of historical code has been working well for a long time.\n\n\n# Imagine there is a large project, how do you do the onboarding ? \n1. Proper documentation in place. How to setup dev env, where to look for important information, ...\n2. Make yourself available for any question that they may have.\n3. Carefully prick their first projects / bugs to work on. Start with simple bugs (text change, colour change, ...).  Then go for a starter project (1 to 2 weeks, self contained, not impacting anybody, that has some impact for them to feel proud).  Then go for a big project that is still non blocking for the rest of the team.\n\n# Work distribution\n#### I have not yet has this kind of experience. \nMaybe a little bit with in Blue Prism with BPCS or the later joiner before I left like Petar.\n\n1. Scope out the work. Not too deep, not too far removed. Scope API endpoints, logical pieces of functionality. Or what are the pages required for front end.\n2. Balance criticality, people preferences and people carrier trajectories.\n\t1. Ensure that works gets done. Hard / long or difficult work is prioritised. The most senior should do these. \n\t2. Balance also preferences. But doing it takes precedences over preferences.\n3. Logistics. If people have to travel, or tight calendar, time zones, ... \n\n\n# Past mistakes: \nDescribe a time when you made a mistake, how did you deal with the repercussion, what lessons did you learn from it ? \n\n#### My mistakes\n1. Chargrid errors not able to resolve.\n2. BPCS when a Senior leader (Colin Redbond) got stressed that the product got too far and decided to submit the research project for scrutiny by all the senior leaders. \n\t1. Deal with repercussion: Answer all their questions and asks. Reduce the scope of the project (to their eyes) make many more presentation. De-armoce the feeling that the project was out of hands. Reassure poeple. \n\t2. What I learn is that: \n\t\t1. Even if you have an idea that is bold and large. Break it down into small pieces. When doing presentation, just talk about the first small target, not the whole idea. People might think too differently from your target, but agree with the first step. Also they will think you want to achieve too much in a short amount of time.\n\t\t2. Find the right people to vouch for your project. Get ppl on your side by making thme participate in the project at a high level. \n\n# Challenging project\nIn BP, the Decision Project was the most challenging one.\nInvolved tight coupling between the product team and research team. Myself and another colleague were making the design decisions that can cope with the requirements coming from the ML techniques that we would be using.\nThe project was delivered very fast, within 4 months.\n\n# Product Outage\n\n# Giving and receiving feed back \n- I have not had very harsh feedback. I do appreciate / like feedback because I always strive to improve. I used to react to feedback to make them clearer to me / justify why things happens that way. But peiople usually think that I am reacting because I do not agree / appreciate the feedback. So I am less reacting to feedback and take them in. I will digest them at a later time.\n\t- I had great feedback from Tautvydas at Callsign:  **TODO ADD SPECIFIC FEEDBACK**\n\t- I had great feedback from John at BP: **TODO ADD SPECIFIC FEEDBACK**\n- Harshest feedback I gave \n\t-  towards my previous manager's manager (Eric Tyree in BP)\n\t- Towards colleagues on engineering guidelines (sometimes when helping debugging code, like with Sandeep at BP)\n\n# Strengths and Improvements\nGOOD:\n- Experiences with Python\n\t- ML\n\t- Engineering\n\t- Making sure that products are well connected together. \n\t- Resolving legacy code for the sake of making the team go forward\n- Soft skills\n\t- Removing ambiguity: Asking questions, diving deep into a project / subject\n\t- \n\nBAD:\n- Work on new languages\n- Improve engineering with regards to design patterns\n- Learn a new language\n- Improve my ability to finish properly a project. I sometime loose motivation. Lately I found that organisation and planning provides enhances \n\n# Comfort Zone\nDescribe getting out of the comfort zone:\n- Presenting ML papers during our reading groups at BP. \n- Starting a project from ground up (BPCS)\n\nGetting out of your comfort zone is a great way to improve. I would like to do it more.\n\n# Reference\nThis notes were taken from: AlgoExpert Behavioural questions preparation videos."},{"fields":{"slug":"/Courses/AlgoExpert/Algo Expert - System Design interview/","title":"Design Facebook News Feed"},"frontmatter":{"draft":false},"rawBody":"# Design Facebook News Feed\nhttps://www.algoexpert.io/systems/workspace/design-facebook-news-feed\n#### Design BEFORE looking at the answer video\n![[Pasted image 20220418202854.png]]\n\n#### Design after looking at the answer video\n![[Pasted image 20220418214721.png]]"},{"fields":{"slug":"/Courses/Educative/Machine Learning System Design/","title":"Example: Video Recommendation"},"frontmatter":{"draft":false},"rawBody":"From https://www.educative.io/courses/machine-learning-system-design/qAqBDXZvpP2\n\n\n```mermaid\ngraph LR\nPS[Problem<br>Statement]\nIM[Identify<br>Metrics]\nIR[Identify<br>Requirements]\nTNE[Train and<br>evaluate<br>models]\nDesign[Design high<br>level system]\nScale[Scale<br>the design]\n\nPS --> IM --> IR --> TNE --> Design --> Scale\n```\nThe 6 basics steps to approach Machine Learning System Design.\n\n### Features\n1. One Hot encoding\n2. [[Feature hashing]]\n3. Crossed feature (aka conjunction), usually used with hashing as well\n4. Embedding: The purpose of embedding is to capture semantic meaning of features\n5. Numeric features:\n\t1. Normalization (0 mean, (-1, 1) range) VS Standardization: (0 mean, divide by std of variable)\n\t2. If feature follows power laws, we can transform using $log(\\frac{1 + v}{ 1 + median(v))})$   \n\n\n### Training pipeline:\n1. Use column-oriented format like Parquet to store data. They have high throughput. Or TFrecord.\n2. Data partitioning to reduce scanning code.\n3. handle imabalnce class distribution:\n\t1. Use **class weights** in loss function.\n\t2. **Naive resampling** of the majority class to reduce its weight\n\t3. **Synthetic resampling** (Synthetic Minority Oversampling Technique - SMOTE)\n4. Right loss function: Using Normalised Cross entropy for CTR is better\n\n### Inference\n1. we can have multiple models trained on different dataset. We should to the right model during inference\n2. Non stationary problems means that we have to do retraining quite often\n3. Exploration vs exploitation: [[Thompson Sampling]]\n\n### Metric evaluation\n\n\n# Example: Video Recommendation\nImportant points: Separate the problem into 2 stages: Candidate Generation and Ranking. \nIf we do ranking on millions of videos, it will not scale. We need to reduce the number of candidates with rough estimation (with focus on **precision**). Then we can use the ranking to predict on fewer videos with the focus on **recall**.\n\n"},{"fields":{"slug":"/Courses/Meta - field guide to ML/Field guide to ML/","title":"Introduction"},"frontmatter":{"draft":false},"rawBody":"# Introduction\nThe right setup is more important than choosing the algorithm.\nStages:\n1. **Problem Definition**\n2. **Data**: how to build dataset\n3. **Evaluation**: How do you define success ?\n4. **Features**\n5. **Models**\n6. **Experimentations**: measure the impact of ML systems in the real world\n\n![[240830049_176529261234363_5106599873009478327_n.png | 700]]\n\n# 1. Define the Problem\nDefine the right problem to solve. Convert the business problem into an ML task.\nThink carefully about what the ML task for your project is.\n\n**What does success look like ?**\nFind simpler tasks that can be good **proxies** for the ultimate goal. \n- Use intermediate events rather than end metric: eg instead of using \"enjoyment of adds\" as the target metric, we can use \"is this add relevant\" to the user. \n- Good intermediate events are a bit less  relevant to the target but can be used to go into the right direction while making it possible to actually measure and train a model on this intermediate event.\n\t- Intermediate events are: less relevant, easier to define, easier to model\n\t- Original events (like enjoyment) are: relevant (to the problem), harder to define, harder to model\n- Properties we want from **proxy events**:\n\t- Does the proxy event happen within a reasonable amount of time from the prediction being made ? (one month is too slow!) - Faster responses makes faster iterations\n\t- Is it too sparse ? 1 Billion examples is nice, but if the positive rate is 1 in a million then this is little data.\n\t- Do features contain information about the event ? ??????\n\t- Is the true desired outcome sensitive to variations ?\n\n\n1. Determine the right task for the project. \n2. Simple is better than complicated\n3. Define your label and training example precisely\n4. Don't prematurely optimise\n\n\n# 2. Data \n1. Data recency and real-time training\n2. Training/prediction consistency\n3. Records and sampling (see [[Handle Imablanced datasets]])\n\nYou can measure performance of the model using data up to day N, then predict the perf on day N+1, N+2, ... and measure **sensitivity** of the performance based on how recently it was last trained.\n\n\n# 3. Evaluation - Offline\nOffline evaluation and Online experimentation\n1. Offline is fast and efficient. Evaluate offline before online\n2. Online if more time consuming but more accurate.\n3. Evaluate both the choice of data and the kind of statistics you calculate\n4. don't be bound to evaluate and train on the same things (the train set can be different from test set)\n5. When evaluating your model, understand where the performance comes from\n\nBaseline model = simplest possible model. Useful to use to compare more complex models.\n\nGold standard of offline evaluation is to split the dataset into Train / Validation / Test. \n\nUsually we do this using K-fold.\n\nPerformance metric should be: \n- **Interpretable**\n- **sensitive to improvements in the model.**\n\n#### Calibration of model\n$calibration = \\frac{\\text{Sum of preds}}{\\text{sum of labels}}$  \nThis is a sanity check. To check that the calibration is the same of both train and test set. It measure that the average prediction is the same btw train and test.\n\n\n# 4. Features \nSelecting features is done with regard to: \n1. The model architecture we are using or want to use\n2. The properties of the data\n3. Special cases\n4. The size of the training data\n\nTypes of features: Categorical / Numerical / Derived.\n\n"},{"fields":{"slug":"/Python/pkg manager/Mamba/","title":"Mamba"},"frontmatter":{"draft":false},"rawBody":"[Documentation](https://mamba.readthedocs.io/en/latest/advanced_usage/package_resolution.html)"},{"fields":{"slug":"/books/All of Statistics - Larry - Unfinished/Chapter 1/","title":"Exercices"},"frontmatter":{"draft":false},"rawBody":"# Exercices\n### Ex 1: \n**Fill in the details of the proof of Th 1.8. Also prove the monotone decreasing case.**\n\n1. Proof that $B_1, B_2, ...$ are disjoint.  \nAssume $\\omega \\in B_i \\cap B_j$ for $i < j$. Then $\\omega \\in A_j$ **and** $\\omega \\in A_i$ **but** $\\omega_i \\notin A_{j-1}$ nor $\\omega_i \\notin A_{i-1}$ by definition of $B_n$. Thus since $i < j$, $A_i \\subset A_{j-1}$ so $\\omega \\in A_{j-1}$. Which is false.   \n**So $B$ are disjoint events.**  \n$\\blacksquare$\n\n2. Proof that $A_n = \\bigcup_{i=1}^n A_i = \\bigcup_{i=1}^n B_i ~~,\\forall n$   \nWe can prove that $lhs \\subset rhd$ and $lhs \\supset rhd$ of the equality, which means equality.  \nFrom right to left:\n$$\\bigcup_{i=1}^n B_i = \\bigcup_{i=1}^n A_i - A_{i - 1}  \\subset \\bigcup_{i=1}^n A_i = A_n$$ And  from left to right:\nIf we define the function $f(\\omega) = min\\{k : \\omega \\in A_k\\}$, then for any $\\omega \\in  A_n$ we have $\\omega \\in B_{f(\\omega)}$. so all elements of $A_n$ are in some $B_k$. => $A_n \\subset B_n$.\n$$\n\\begin{align}\nB_n \\subset A_n \\text{ and } A_n \\subset B_n \\Leftrightarrow A_n = B_n & \\\\ \n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t & \\blacksquare\n\\end{align}\n$$\n\n\n\n\n\n3. Proof theorem 1.8 for monotone decreasing events.  \nIf $A_1 \\supset A_2 \\supset ... A_n$ then the complement is monotone increasing $A_1^c \\subset A_2^c \\subset ... A_n^c$. As we proved the theorem for monotone increasing events, we have \n$$\n\\begin{align}\n&P(A_n^c) \\underset{n \\rightarrow \\infty}{\\rightarrow} P(A^c) & \\\\\n\\text{or}&& \\\\\n&P(A_n) = 1- P(A_n^c) \\underset{n \\rightarrow \\infty}{\\rightarrow} 1 - P(A^c) = P(A)& \\\\\n&&\\blacksquare \n\\end{align}\n$$\n\n### Ex 2\nProve the statements of equation 1.1: \n1. $P(\\emptyset) = 0$\n2. $A \\subset B \\rightarrow P(A) \\leq P(B)$\n3. $0 \\leq P(A) \\leq 1$\n4. $P(A^c) = 1 - P(A)$\n5. $A \\bigcap B = \\emptyset \\rightarrow P(A \\bigcup B) = P(A) + P(B)$\n\nSolutions: \n1. If we partition the space into $\\{\\Omega, \\emptyset\\}$ we have $P(\\Omega \\bigcup \\emptyset) = P(\\Omega) + P(\\emptyset)$ (axiom 3). Or $\\Omega \\bigcup \\emptyset = \\Omega$ so $P(\\Omega) + P(\\emptyset) = P(\\Omega) \\rightarrow P(\\emptyset)= 0 ~ \\blacksquare$ \n2. $P(A) = P(B) - P(B - A)$ so $P(A) \\leq P(B) \\blacksquare$ \n3. $P(\\Omega) = 1 \\rightarrow P(\\Omega) = P(A) + P(A^c) = 1 \\rightarrow P(A) \\leq 1  \\blacksquare$ \n4. $P(A) + P(A^c) = P(\\Omega) = 1 \\rightarrow P(A) = 1 - p(A^c) \\blacksquare$ \n5. We know that $P(A \\bigcup B) = P(A) + P(B) - P(A \\bigcap B)$ or $P(A \\bigcap B) = P(\\emptyset) = 0$ $\\blacksquare$ \n\n\n### Ex 3\nLet $\\Omega$ be a sample space and let $A_1, A_2, ...$ be events. Define $B_n = \\bigcup_{i=n}^\\infty A_i$ and $C_n = \\bigcap_{i=n}^\\infty A_i$\n1. Show that $B_n$ is monotonic decreasing and $C_n$ is monotonic increasing.\n2. Show that $\\omega \\in \\bigcap_{n=1}^\\infty B_n$ if and only if $\\omega$ belongs to an infinite number of the events $A_i$\n3. Show that $\\omega \\in \\bigcup_{n=1}^\\infty C_n$ if and only if $\\omega$ belongs to all of the events $A_i$ except possibly a finite number of those events.\n\n**Solutions:**\n\n1. $B_{n-1} = A_{n-1} \\bigcup B_n \\supset B_n$ so B is monotonic decreasing. Similar for C.\n1. (My )Solution by contrapositive. Let's have $\\omega \\in \\bigcap_{n=1}^\\infty B_n$. Assume $\\omega \\in \\{A_j \\bigcap ... A_k\\}$ a finite set where $j < k$. Then $\\omega \\notin A_{k+1}$. Or by construction $\\omega \\in B_{k + 1} = B_k \\bigcap A_{k+1}$ This implies that no such $\\omega$ can satisfy this property. So we cannot find a $k$ for which $\\omega \\notin A_k$ for all $k$.   \n**Online solution:** Assume that $\\omega \\in \\bigcap_{n=1}^\\infty B_n$. Then, for every $n$, $\\omega \\in B_n$, so for every $n$ there is a $m \\geq n$ such that $\\omega \\in A_m$,. This implies there is an infinite number of such events $A_m$.\n1. Let's prove the contrapositive.\n-   Assume that ω does not belong to an infinite number of events Ai. Then, for every n, there is a m≥ such that ω∈Amc, and so ω is not in Cn. Since ω is not in none of the Cn's, it is not in the union of all Cn's either.\n-   Assume that ω is not in the union of all Cn. This implies that ω is is not in any event Cn. This implies that, for every n, there is a m≥n such that ω is not in Am. This implies that there is an infinite number of such events Am.\n\n### Ex 4\nLet $\\{A_i:i \\in I\\}$ be a collection of events where $I$ is an arbitrary index set. Show that \n$$ \\text{1- }~(\\bigcup_{i \\in I} A_i)^c = \\bigcap_{i \\in I} A_i^c ~\\text{  and 2- } ~ (\\bigcap_{i \\in I} A_i)^c = \\bigcup_{i \\in I} A_i^c$$\nLet's prove it for only 2 sets $A$ and $B$.  \n$$(A \\cup B)^c = \\Omega  -  (A \\cup B) = \\Omega - A - B = A^c - B = A^c \\cap B^c$$\nIf $B$ is the collections of $A_i$ without one element (that we chose to be $A$) then by transitivity, we have the first result.   \n\nFor the second result\n$$\n\\begin{align}\n                & ~ \\omega \\in (\\bigcap_{i \\in I} A_i)^c \\\\\n\\Leftrightarrow & ~ \\text{ not } (\\omega \\in \\bigcap_{i \\in I} A_i) \\\\\n\\Leftrightarrow & ~ \\text{ not } (\\forall i \\in I, \\omega \\in A_i)\\\\\n\\Leftrightarrow & ~ \\exists i \\in I, \\omega \\notin A_i \\\\\n\\Leftrightarrow & ~ \\exists i \\in I, \\omega \\in A_i^c \\\\\n\\Leftrightarrow & ~ \\omega \\in \\bigcup_{i \\in I} A_i^c \\\\\n\\end{align}\n$$\n\n### ex 5\nSuppose we toss a fair coin until we get exactly two heads. Describe the sample space $S$. What is the probability that exactly $k$ tosses are required ? \n\nSolution: \nLet's call the event $H_{i, j}$ the event of $j$ coin toss, where toss number $i$ and $j$ are heads and all other tosses are tails.  \nThen $S = \\{H_{i, j}, i < j, j\\in \\mathbb{N}\\}$.   \nWe have a uniform distribution over the finite sample space $\\Omega = \\{\\text{head , tail}\\}^k$.  By definition of the probability on Finite sample spaces, we have \n$$ P(H_{i,k}) = \\frac{|H_{i, k}|}{|\\Omega|} = \\frac{k-1}{2^k} $$ Where $|H_{i, k}| = k - 1$ because there are only k-1 possible events that have K toss and ends with a head.\n\n\n### ex 6\nLet $\\Omega= \\{0,1,2,…\\}$. Prove that there does not exist a uniform distribution on $\\Omega$ (i.e. if $P(A)=P(B)$ whenever $|A|=|B|$ then $P$ cannot satisfy the axioms of probability).\n\n**Solution**:\n Assume that such a distribution exists, and let $P(\\{1\\})=p$. Since the distribution is uniform, the probability associated with any set of size 1 is p, and the probability associated with any set of size n is np.\n\n-   If p>0, then a finite set A of size |A|=⌈2/p⌉ would have probability value P(A)=⌈2/p⌉p≥(2/p)p=2, which is greater than 1 -- a contradiction.\n-   If p=0, then any finite set A must have P(A)=0. But then P(Ω)=∑iP({i})=∑i0=0, instead of 1 -- a contradiction.\n\n### ex 7\n\n"},{"fields":{"slug":"/books/Design Data Intensive Apps/Ch 4 Encoding and Evolution/","title":"Formats for Encoding Data"},"frontmatter":{"draft":false},"rawBody":"# Formats for Encoding Data\n# Modes of Dataflow\n## Dataflow through databases\n## Dataflow through Services: REST and RPC\nWhen we need 2 services to communicate over a network, we need to define the schema of the requests and responses. Data will be exchanged between the client and the server. If HTTPS is used to transmit information, we call this a WebService.\n\nSo far, this is similar to the communication btw a client and a database. However, in the case of services we do not want the client to run any types of query (contrary to the database). Services exposes specific API that allows inputs and outputs that are predetermined by the business logic. This restriction provides a degree of encapsulation.\n\n#### REST\n**REST** and **SOAP** are two popular method to agree on what the communication Schema for WebServices looks like. But SOAP is falling out of favour so forget about it.\n\n#### RPC\n> **RPC**: Remote procedure call: tries to make a request to a remote network service look the same as calling a function or method in your programming language, within the same process. Such abstraction is called *\"location transparency\"*.\n\nThe book mentions many downsides to RPC. I disagree with these points. The main argument is that the author combines the fact that RPC makes a server call look like a regular function call. I believe that the programmer / user of the API knows that a RPC call does not behave in the same way as a regular function call and is faced with the same issues that would arise nonetheless when using REST.\n\nThe issue related to lost responses is the same between REST and RPC. \nOverall RPC is more about the simplicity of sending the data to the other server. It is miss-leading to think that the user / programmer should not benefit from the nice syntax of native function to perform these communication because these function would behave differently. It is up to the user to know what exception can be raised by looking at the documentation. \n\nHowever, it is true that the user has to know how to handle Time out and lost responses or idempotent operations. But this knowledge should be acquired when using both REST and RPC.\n\n\n#### REST vs RPC\nCustom RPC protocols with a binary encoding format can achieve better performance than something generic like JSON over REST. But REST API are easier for experimentation and debugging. \n\n> I recommend to use RPC for internal services which are owned by the same company. It makes is simpler to maintain the schema as well. REST is preferable for public endpoints. \n\n**I enjoy to use gRPC because the schema is enforced by code**. In the case of REST, one endpoints uses a specific schema and the schema is enforced via testing. Which is much more brittle and some edge cases could be missed. It is preferable to use gRPC which requires the creation of a Protocol Buffer schema which both clients and server have to use. \n\n\n#### Message-passing Dataflow\n\n```mermaid\ngraph LR;\n\nC[Client] --> B(Broker) --> S[Server]\n```\n\nUsing a Message broker has several advantages: \n- Improve sys reliability: acts as a buffer if the recipient is unavailable. \n- Prevent lost msg: automatically redeliver messages to a process that has crashed.\n- Decouples sender and recipient: \n\t- the sender does not need to know PORT and IP of receiver.\n\t- The sender does not know the receiver or how many.\n- Allows one msg to be send to several recipients.\n\nDownside is that the communication becomes one way. if you want to have it two way, you need to add a separate channel. Thus this communication pattern is `asynchronous`.\n\n##### Distributed actor frameworks\n[...]\n\n\n# Summary\nAs services have to communicate between themselves, we looked at how to encode messages (JSON, binary, with schema...). \nIn order to update services, we have a to still be able to handle older and newer message format. Being `backward` and `forward` compatible allows to gradually update components instead of all services at once. However, it is also a bit of a challenge. Schema evolution is something to be mindful of but [[Thrift]] and [[Protocol Buffers]] helps you to achieve it.\n\n[[Rolling updates]] allow new versions of a service to be released without downtime. Since different services of different versions will coexists, we need backward and forward compatibility.\n\nWe have seen how to communicate between services: \n- Databases\n- REST / SOAP \n- RPC \n- Asynchronous message passing using brokers or actors.\n\n"},{"fields":{"slug":"/Python/pkg manager/PIP/","title":"PIP"},"frontmatter":{"draft":false},"rawBody":"Install a package by from the default pip repository. This is usefull to bypass local pip forwarding to a secure Azure Pip repo when you do not have authentication tools downloaded yet.\n```shell\npip install artifacts-keyring --index-url https://pypi.org/simple\n```\n"},{"fields":{"slug":"/books/Design Data Intensive Apps/Ch 5 Replication/","title":"Ch 5 Replication"},"frontmatter":{"draft":false},"rawBody":"We want [[Replication]] to handle node failure, improve scalability and latency.\n\nIn this chapter we assume the dataset can fit within a single machine.\n\nThe hard part of replication is **handling changes** to replicated data. \nThere are multiple algorithms for replicating changes between nodes: \n- Single-leader\n- Multi-leader\n- Leaderless\n\nTrade-offs to consider: **synchronous / asynchronous** and **how to handle failed replicas**.\n\n## Leaders and followers / Leader based replication / Single leader\n```mermaid\ngraph TD;\n\nU1(User) -- \"read-write queries\" -->L[\"Leader Replica\"] \nL -- \"data change\" --> R1[\"Follower Replica\"]\nL -- \"data change\" --> R2[\"Follower Replica\"]\nU2(User) -- \"read-only queries\" --> R2\n```\n\n### Synchronous vs Asynchronous replication\nIn a **Synchronous** case, the leader waits until the follower replica has confirmed that it received the write before reporting success to the user and before making the write visible to the clients.\nIn an **Asynchronous** case, the leader sends the message to the follower replica but does not wait for a response from the follower.\n\nUsually, we want to have **one** follower to be synchronous and other replicas are asynchronous. AKA **semi-synchronous**. \n\n### Setting up new followers (replica)\n1. Take snapshot of Leader's DB at some point in time. \n2. Copy snapshot to the new follower node\n3. The follower requests from leader all the data changes that happened since the snapshot was taken.\n4. When done, the follower has `caught up` and is now available.\n\n### Handling node outages\n- Follower failure: recovers in the same way as spinning up a new follower\n- Leader failure: **TOUGH !**\n\n### Implementation of replication logs\nDifferent approaches\n1. Statement based replication (not used anymore)\n2. Write ahead log (WAL) shipping\n3. Logical (row-based) log replication\n4. Trigger-based replication\n\n\n### Replication Lag\n> [!tldr] Eventual Consistency\n> With asynchronous followers, due to communication issues or delay, it is possible that a replica has a delayed state compared to the leader, which can last idealy a few seconds but can be more than that (minutes !). *Eventually* the follower will get up to date.\n\nDue to eventual consistency, a user sending data to the DB, could see that the data got lost if the next request reads from a follower that has not been updated yet. This is awful because the user thinks the data has been lost.\n\nSolutions: **Read your own writes**:\n- Read from the leader when the user did a write, if not read from followers\n- Use other heuristics. Check what is the replication lags, ...\n- Collect timestamp of latest write from client and decide which replica to use. \n\n### Monotonic reads\n\nIf a user queries followers with increasing replication lags, they believe the data is regressing !\n> [!tldr] Monotonic reads\n> Enforce that the user always see the same state or a later state of the database.\n> It is a lesser guarantee than *strong consistency* but stronger than *eventual consistency*\n> eventual consistency < monotonic reads \n\n\n\n"},{"fields":{"slug":"/books/Design Data Intensive Apps/test slides/","title":"Hello world"},"frontmatter":{"draft":false},"rawBody":"# Hello world\nPresented by #ThomasSajot\n\n\n> [!FAQ] Thomas \n> Some date\n\n\n---\n\nHello you\n\n```python\nimport pandas as pd\n\ndef foo(a: str) -> int:\n\treturn int(a)\n\nif __name__ == \"__main__\":\n\tfoo('120')\n```\n\n---\n# Something else\n\n- [x] Some important stuff\n- [ ] TODO\n- [ ] These\n- [ ] Title\n- [ ] Are \n- [ ] very \n- [ ] Big\n- [ ] Don't you think ? \n\n---\n##### One Tool to rule them all\n![](Screenshot%20(9).png)\n\n---\nSome mermaid for the ending\n\n\n```mermaid\ngraph TD\nRR[Research Repo]\nSR[Server Repo]\nAA[Azure Artifacts]\nDI[Docker Images]\n\nRR -- \"(1) push code to master<br> and pip deploy to\" --> AA\nSR -- \"(2) pip install from \" --> AA\nSR -- \"(3) Build and Deploy Container\" --> DI\n```"},{"fields":{"slug":"/Courses/Coursera/Learning how to learn/Coursera - Learning How to Learn/","title":"Week 1"},"frontmatter":{"draft":false},"rawBody":"# Week 1\n### 2 States of the mind: Focused and Diffused. \n- The focused mode is when we are focusing on a problem, intensely using pre-built thought pathways in our minds. \n- The diffused mode is when we are not focusing (doing sport, sleeping, cooking, ...). This disengagement of the mind from a particular subject allows higher level abstraction to be built\n\n### The are 2 (main) state of memory: working and Long term memories\n- Working memory is used for short term tasks. We have up to 4 slots or `chunks` of information we can store in our working memory. These slots are temporary so we will forget them pretty quickly.\n- Long term memory is used for keeping information for a long period of time. It is not easy to add new memories there.\n\nTo build knowledge is to create `stable` pathways in our minds. Moving thought processes and memories from short term to long term srtorage. \n\n\n### Learning habits\nTo learn something properly, we need to:\n1. work hard on it. Making the effort to experience new pathways. Being passive is way less productive than active in the learning process (for example watchiung a video <<<< doing the implementation yourself). \n2. Then relax, sleep, meditate, cook, do something else. This freeze the attention and allows your brain to assimilate the information. During this step the brain will digesty the information and build new neurological pathways. Sleeping is very important to build this pathways.\n3. Repeat 1. and 2. ! This process helps to migrate the memory from working to long term storage.\n\n### The Pomodoro technique\nIf you are having a hard time starting a new study, because it is hard, especially if you procrastinate, the pomodoro techniques helps you: \n1. Use a timer, give yourself 25 min (you can adapt) of study the material\n2. Take a break. This gives you a (small) reward after the effort. Which motivates you further and reduces the pain / angst again this study\n\n## Take away\n- Efficient learning involves going back and forth between Focused and Diffused modes.\n- Metaphors provide powerful techniques for learning \n- Learning something difficult takes time. Acknowledge it ! \n- Practice makes knowledge permanent\n- You have to add rest in between focus events (use pomodoro if need be)\n- Using `Spaced repetition` is very good to migrate information from Working to Long term memory. The Anki app is one tool do follow Spaced repetition.\n- Sleeping is very important to learn:\n\t- Clean brains \n\t- Is healthy \n\t- Clears unused pathways and strengthen the others ! \n\nStudying something just before going to bed is also powerful. Your brain will work harder on it over night.\n\n__________________\n__________________\n\n\n# Week 2\n\n## Chunking\nA chunk is a compact packages of information that your mind can easily access.  \nChunking is a logical package of pieces of information to make them easier to work with this concept.\n\nThe human can sustain 4 chunks in his working memory.\nA chunk is a network of neurones that are used to firing together.  \n\n### How to form a chunk ? \n1. You need to be in focus mode. no TV or anything else. Chunking relies on building new patterns using already existing one. This does not happen well if we are not focused.\n2. You need to understand the concept you are looking to chunk. This can be done by alternating btw Focused and Diffused mode. \n\t**But understanding how a problem is solved does not mean you create a chunk**. You need to **do it**, and make sure you can use this knowledge.\n3. It is important to build **context**. which is about surrounding information connecting them to the chunk. For example doing problems related to the original chunk. This will create new neural connections to the chunk you are building which makes it easier to come back to in different situations.\n\nLearning is a 2 step process.. Chunking is a `bottom-up` way to build a chunk (one by one, to learn a specific thing). Practice will increase the number of chunks related to each other. Using the big-picture (diffused mode) you will be able to connect chunks together. This builds the **Context** that relate chunks into one another.\n![[Pasted image 20220124212910.png|600]]\n\n![[Pasted image 20220124212611.png|500]]\n\n\n## Illusions of competence in Learning\n### Importance of Recall\nIt is better to recall the information of a text just read rather than re-reading the material. The learning is faster and much deeper.\n**The retrieval process itself is useful for remembering.**\n\n### Illusion of competence\nReading the solution of a solution, and thinking `I understand what they did here`. This is not knowing and you will not improve.\n\nIn the same way, reading google solutions will keep the illusion of competence.\n\nre-reading and highlirting too much text promotes ILLUSION OF Competence. \n\n--> **Testing yourself promotes long term learning via stabilising activations patterns in a large brain area.**\n--> Do Mini testing !!!!!!!!\n--> You can train to recall your information in different rooms in order to dissociate the knowledge from where you learned it.\n\n### Value of making mistakes\nMistakes are useful during mini testing. These mistakes help correct your thinking.\n\n\n\n## Seeing the bigger Picture\n### Library of chunks\n\nHaving many chunks helps to have a library of knowledge. You need to practice with growing chunks in order to remember and srtill be able to recall / use them.\n\n### Two ways to solve problem:\n1. Sequential thinking, using the focused mode\n2. or Holistic (global) approahc. Using intuition / Diffuse modes. A solutuon the diffuse mode provide should be verified in a focused mode.\n\n\n## Interleaving\n- **Overlearning** is when you work too much on the same material within the **same session** . It is still usefull to master something, but be careful when to use it.\n- Doing also the same thing you already know in further sessions is not good and give an illusion of competence again. \n- We need to do a **Deliberate practice** which is about balancing your study, studying what you find difficult.\n- Einstellung: an idea you already have in mind prevent a better idea or solution of being found.  You need to make sure that your original intuition is not false.\n- **Interleaving**: Once you have learned a new material / chunk and are not very familiar with it, interleaving is about balancing practicing with problems of different types around this chunk.\n\nInterleave is very important. it helps to build this creative pattern that you can rely on in new problems.\n\n\n\n## Summary\n- Chunks are pieces of information that are bound together through use and through meaning.\n- Chunk is a single easy item to access that uses one slot in the working memory. \n- Chunks are build with Focused attention, understanding and practice\n- Simple Recall (remembering the key points without looking at the page) is a powerful way to build chunks. \n\t- Bonus to recalling memory in different locations.\n- `Transfer` is the fact that you can relate chunks over different domains. They will help you learn new chunks\n- Interleaving learning by practicing your choice of concepts and techniques. This helps at becoming flexible with which chunks to use\n- Test yourself !!! this is a good way of avoiding the illusions of competence.\n- Mistakes are good.\n- Avoid practicing at easy stuff.\n- Einstellung is when an idea you already have in mind prevents a better idea / solutions from being found.\n- L:aw of Serendipity: just learn one thing, and you will see that the next one will be a little bit simpler to learn.\n\n\n### Notes on interview with Norman Fortenberry  @ MIT \n\n- making sure to have the \n\n__________\n__________\n# Week 3\n\n## Tackling Procrastination\nRemember the pomodoro technique (25 min work / few minutes relaxing)\n\n>**Important** Willpower is very expensive for the brain, so you should avoid using willpower against procrastination unless very necessary. There are other approaches that do not involve willpower.\n\nProcrastination is a habit. So improving procrastination will help in many domains. \n\nWhen you procrastinate, you feel better--but only temporarily.  In this, procrastination shares common features with addiction.\n\nIt is possible to control this habit, which are not intuitive to find. \n\n\n### Habits\n**Habits** is a \"zombie\" mode, it saves energy. It can be good or bad.\n\nHabits have 4 parts: \n1. The cue: triggers the routine\n2. The routine: is the zombie mode. The routine that your brain is used to go into when getting the cue.\n3. The reward: a habit gets reward. For example procrastination gives a nice reward. Good habits can be rewarding. This is a good way to escape procrastination by giving good reward on good habits\n4. The belief: habits have power because of your belief in them. For example belief that you cannot study so you have a habbit of procrastination.\n\n\n### Tricks for learning\nIt is fine to experience some bad feeling before learning something. But the important thing is to manage those feeling.\nFor example `get on with it.`. \n\nThere is another helpful way to reframe the work: Process vs Product: \n- **Process**: is the flow of time and the process associated with the flow of time: I am going to work for 25 min.\n- **Product**: My goal is to finish this assignment.\n\n**The idea is to NOT focus on product but process.** Your attention should be devoted to build habits with good processes. \nWho cares if you learned the material in one session ? The goal is to get there.\n\nThe easiest way to focus on process is to focus on the Pomodoro technique for example. \nHaving the process has a \"goal\", you don't judge yourself.\n\nThe hard part is to avoid distraction, learn to ignore them and put yourself in a position of focus (earplugs, isolated, ...)\n\n**Processes** related to simple habits. Habits that coincidentally allow you to do the unpleasant tasks that need to be done. \"just work 25 min\".\n\n### How to harness your habits to avoid procrastination and minimize use willpower\nThe trick to overwritting a habbit is to look to change a reaction to a cue. The only willpower you nmeed is to change the response to a cue.\n\n1. **The cue**: recognise what launches you into procrastination. \n\tTriggers are usually: location, time how you feel, reactions (txt msg, ...). You cna shut down your cellphone or isolated yourself.\n2. **The routine**:  The brain reacts to this cue and wants to go into the routine. \n\tThe key to change this is to have a **plan** to help cope. For example isolate yourself. The pomodoro technique is good to shift your reaction.\n3. **reward**: why are you procrastinating ? Add a new reward for your work. **It helps to add a new reward if you want to create a new habit**\n4. **The belief**: Belief in your new system works ! Having a new community can help as well. A culture with like minded ppl will help to remember the values that you should believe i (like I can learn this topic.)\n\n### Planner journal: Juggling life and learning.\nOnce  a  week write a list of key tasks. And every day write a daily task list (the evening before). \nWriting it the evening before will help you to solve these during your sleep !\n\nIf you don't write them down, these tasks list will use working memory.\n\n**Tasks can be process oriented !** Items can also be product oriented, but because they are achievable. \nThere is a goal as well. For example stop working at 5pm.\n \n mixing other tasks with your learning is helpful. These can beused as relaxation times.\n\n The more you do the tasks the more you get used to how many times can be achieve.\n\n **Planning your quitting time is very important**. \n\n Try to work on a most important and most disliked task first. Like pomodoro when you wake up.\n\n Planning well is part of trying. The law of serendipityL: lady luck favors the one who tries.\n\n ### Summary\n - keep a planner journal \n - commit yourself to certain routines and tasks each day \n - get rewards after your effort \n - delay rewards until you finish a task\n - watch for procrastination cues\n - Gain trust in your new system. You want to work hard when it is time to work. And relax without bad feelings when it is time to relaex.\n- Eat your frog first. Do the harder tasks in the morning.\n\n\n## Memory\nYou can use a use another modality (usually visual) to remember a concept. For example: the formula $f = ma$, force equal mass times acceleration. could be `a flying mule arse` (FMA)\n\nThe image needs to be memorable.\n\nIndex card, spaced repetition can be helpful.\n\n\nUsing handwritting help to better learn these information.\n\nAgain interleaving the learning: ie remembering different concept is good. \nTraining for a few days is important. Increase spacing between each remembering effort.\nANKI is a great tool for this.\n\n\n### Long term memory\n\n\nWhenever you recall memory, you will remember them better. this is called `reconsolidation`. Reconsolidation also happens over sleep. \n![[Pasted image 20220213124253.png|400]]\n\nThis is why it is better to take time to learn something , rather than cram.\n\n\n## Creating meaningful groups and the memory palace technique\n\nMemory palace technique is good to remember a list of unrelated things.\n\n\n# Week 4\n## Tips for becoming a learner\n1. Exercises often ! Exercises helps new acquired neurones to stay and not decay ! \n2. Practice makes perfect, at certain times. (Not sure what was the point)\n\t1. Critical period for learning langugaes is up to purberty\n\n## Renaissance learning\n\nLearning is not just adding knowledge all the time. Sometimes it feels like the brain \"unlearned / forgot\" the previous stuff. They don't make sense anymore. This can happen because the brain is **restructuring** the knowledge ! So this is good. A step backward in the understanding is normal. When you emerge from these temporary frustration, you will notice that you know much more.\n\nPersonal experience with learning Naive Bayes classifier. It took me a few sessions of a few days each to understand it. Everytime, at the beginning of a new sessions, it seamed that everything was blurry.\n\n## Visual metaphor or analogy\nBest way to remember a memory is to use a metaphor or analogy. Even better if it is visual because we have a very good visual memory.\n\nMetaphors are very good to get free from [[Einstellung]] ! (being stuck on a problem). \n\n\n## Repetition is key\nYou need to repeat something. And you need to make sense out of the knowledge in order to acquire it.\n\nAt some point, trying to  understanding \"why\" you do what you do is counter producting. You need to accept higher level concept when you arer confortable with them.\n\nExample is einstellung. If you are using all your chuncks to keep track of all the explanations of something in your mind prevents from having space for new ideas at higher level concepts.\n\n\n**Imposter syndrome** is very common. It is normal to feel that way.\n\n\n## The value of team work\n...\n\n## Test tips:  Test checklist\nTesting yourself is a very powerful learning effort. This is **FUNDAMENTAL**. \nTesting is one of the procedure of learning.\n\nHere is a checklist for test taking: \n- Did you make a serious effort to understand the text? \n\tJust hunting for relevant worked-out examples doesn't count. \n\n- Did you work with classmates on homework problems or at least check your solutions with others? \n- Did you attempt to outline every homework problem solution before working with classmates? \n- Did you participate actively in homework group discussions, contributing ideas and asking questions? \n- Did you consult with the instructor or teaching assistants when you were having trouble with something? \n- Did you understand all your homework problem solutions when they were handed in? \n- Did you ask in class for explanations of homework problem solutions that weren't clear to you? \n- If you had a study guide, did you carefully go through it before the test and convince yourself you could do everything on it? \n- Did you attempt to outline lots of problem solutions quickly without spending time on the algebra and calculations? \n- Did you go for the study guide and problems with classmates and quiz one another? \n- If there was a review session before the test, did you attended and asked questions about anything you weren't sure about? \n- Lastly, did you get a reasonable night's sleep before the test?\n\n\n## Test tips: Hard start - Jump to easy\nIn a test: start on a hard problem, work on it a little / until you get stuck  (it can be just a few minutes !). Then jump to easy problems and come back to the hard one. \n\n\n "},{"fields":{"slug":"/Courses/Coursera/Learning how to learn/Einstellung/","title":"Einstellung"},"frontmatter":{"draft":false},"rawBody":"Being stuck on a problem because you are unable to approach it with a different angle."},{"fields":{"slug":"/Python/pkg manager/Conda/","title":"Main Commands"},"frontmatter":{"draft":false},"rawBody":"# Main Commands\n\n| Description               \t| Command                             \t\t|\n|-----------------------------\t|----------------------------------------\t|\n| Create env from a yml file  \t| `conda env create -f environment.yml`  \t\t|\n| Create env from scratch     \t| `conda create --name SAIL python=3.7`  \t\t|\n| Remove env                  \t| `conda env remove --name SAIL`           \t\t|\n| Conda set number of threads \t| `conda config --set default_threads 6` \t\t|\n| Conda update env from file    | `conda env update --file environment.yml --prune `|\n| Conda export env without build specific info | `conda env export --no-builds` |\n\n\n# Complex commands\n\n#### Generate an environment file without all the packages, and without the pip section and the prefix section\n```shell  \nconda activate SAIL;conda env export --from-history > environments/tmp-words;\nconda env export --no-builds | sed '/- pip:/,$d' > environments/tmp-generated-base;\necho \"# GENERATED FILE, please see environments/README.md\" > environments/generated-base.yml;\ngrep -Fw -f environments/tmp-words environments/tmp-generated-base >> environments/generated-base.yml;\nrm environments/tmp-words environments/tmp-generated-base;\n```"},{"fields":{"slug":"/Courses/Coursera/Learning how to learn/Habits/","title":"Habits"},"frontmatter":{"draft":false},"rawBody":"4 stages to a habit\n1. Cue\n2. Routine\n3. Reward\n4. Belief\n\n\nExample for running\nExample of [[Procrastination]]"},{"fields":{"slug":"/placeholder/","title":"This Is a Placeholder File for Mdx"},"frontmatter":{"draft":true},"rawBody":"---\ntitle: This Is a Placeholder File for Mdx\ndraft: true\ntags:\n  - gatsby-theme-primer-wiki-placeholder\n---\n"}]}}}