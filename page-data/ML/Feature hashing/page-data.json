{
    "componentChunkName": "component---node-modules-gatsby-theme-primer-wiki-src-templates-post-query-js",
    "path": "/ML/Feature hashing/",
    "result": {"data":{"mdx":{"id":"188f8abf-5299-53f5-8689-c65c05c6d9fa","tableOfContents":{},"fields":{"title":"Feature Hashing","slug":"/ML/Feature hashing/","url":"https://thomassajot.github.io/brain-dam/brain-dam/ML/Feature hashing/","editUrl":"https://github.com/thomassajot/brain-dam/tree/main/ML/Feature hashing.md","lastUpdatedAt":"2022-05-27T09:02:01.000Z","lastUpdated":"5/27/2022","gitCreatedAt":"2022-05-27T09:02:01.000Z","shouldShowTitle":true},"frontmatter":{"title":"","description":null,"imageAlt":null,"tags":[],"date":null,"dateModified":null,"language":null,"seoTitle":null,"image":null},"body":"var _excluded = [\"components\"];\n\nfunction _extends() { _extends = Object.assign ? Object.assign.bind() : function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar _frontmatter = {};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, _excluded);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"p\", null, \"Feature hashing, called the hashing trick, converts text data or categorical attributes with high cardinalities into a feature vector of arbitrary dimensionality.\"), mdx(\"h3\", {\n    \"id\": \"benefits\"\n  }, \"Benefits\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Feature hashing is very useful for features that have high cardinality with hundreds and thousands of unique values. Hashing trick is a way to reduce the increase in dimension and memory by allowing multiple values to be present/encoded as the same value.\")), mdx(\"h3\", {\n    \"id\": \"feature-hashing-example\"\n  }, \"Feature hashing example\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"First, you chose the dimensionality of your feature vectors. Then, using a hash function, you convert all values of your categorical attribute (or all tokens in your collection of documents) into a number. Then you convert this number into an index of your feature vector. The process is illustrated in the diagram below.\")), mdx(\"p\", null, \"fox3brown4quick4the0\"), mdx(\"p\", null, \"An illustration of the hashing trick for desired dimensionality of 5 for the originality of K of values of an attributes\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, \"Let\\u2019s illustrate what it would look like to convert the text \\u201CThe quick brown fox\\u201D into a feature vector. The values for each word in the phrase are:\"), mdx(\"pre\", {\n    parentName: \"li\"\n  }, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"the\\xA0=\\xA05\\nquick\\xA0=\\xA04\\nbrown\\xA0=\\xA04\\nfox\\xA0=\\xA03\\n\"))), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, \"Let define a hash function, hh, that takes a string as input and outputs a non-negative integer. Let the desired dimensionality be 5. By applying the hash function to each word and applying the modulo of 5 to obtain the index of the word, we get:\"), mdx(\"pre\", {\n    parentName: \"li\"\n  }, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"h(the)\\xA0mod\\xA05\\xA0=\\xA00h(quick)\\xA0mod\\xA05\\xA0=\\xA04h(brown)\\xA0mod\\xA05\\xA0=\\xA04h(fox)\\xA0mod\\xA05\\xA0=\\xA03\\n\"))), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, \"In this example:\"), mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"h(the) mod 5 = 0\"), \" means that we have one word in dimension \", mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"0\"), \" of the feature vector.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"h(quick) mod 5 = 4\"), \" and \", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"h(brown) mod 5 = 4\"), \" means that we have two words in dimension \", mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"4\"), \" of the feature vector.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"h(fox) mod 5 = 3\"), \" means that we have one word in dimension \", mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"3\"), \" of the feature vector.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"As you can see, that there are no words in dimensions 1 or 2 of the vector, so we keep them as 0.\"))), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, \"Finally, we have the feature vector as: \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"[1, 0, 0, 1, 2]\"), \".\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, \"As you can see, there is a collision between words \\u201Cquick\\u201D and \\u201Cbrown.\\u201D They are both represented by dimension 4. The lower the desired dimensionality, the higher the chances of collision. To reduce the probability of collision, we can increase the desired dimensions. This is the trade-off between speed and quality of learning.    \"))), mdx(\"blockquote\", null, mdx(\"p\", {\n    parentName: \"blockquote\"\n  }, \"Commonly used hash functions are \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://en.wikipedia.org/wiki/MurmurHash\"\n  }, \"MurmurHash3\"), \", \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://en.wikipedia.org/wiki/Jenkins_hash_function\"\n  }, \"Jenkins\"), \", \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://en.wikipedia.org/wiki/List_of_hash_functions\"\n  }, \"CityHash\"), \", and \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://en.wikipedia.org/wiki/MD5\"\n  }, \"MD5\"), \".\")), mdx(\"h3\", {\n    \"id\": \"feature-hashing-in-tech-companies\"\n  }, \"Feature hashing in tech companies\", mdx(\"a\", {\n    parentName: \"h3\",\n    \"href\": \"https://www.educative.io/courses/machine-learning-system-design/q2AwDN4nZ73#Feature-hashing-in-tech-companies\"\n  }, \"#\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Feature hashing is popular in many tech companies like \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.booking.com/\"\n  }, \"Booking\"), \", \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.facebook.com/\"\n  }, \"Facebook\"), \", \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.yahoo.com/\"\n  }, \"Yahoo\"), \", \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://yandex.com/\"\n  }, \"Yandex\"), \", \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://avazu.com/home/\"\n  }, \"Avazu\"), \" and \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.criteo.com/\"\n  }, \"Criteo\"), \".\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"One problem with hashing is collision. If the hash size is too small, more collisions will happen and negatively affect model performance. On the other hand, the larger the hash size, the more it will consume memory.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Collisions also affect model performance. With high collisions, a model won\\u2019t be able to differentiate coefficients between feature values. For example, the coefficient for \\u201CUser login/ User logout\\u201D might end up the same, which makes no sense.\")), mdx(\"blockquote\", null, mdx(\"p\", {\n    parentName: \"blockquote\"\n  }, \"Depending on application, you can choose the number of bits for feature hashing that provide the correct balance between model accuracy and computing cost during model training. For example, by increasing hash size we can improve performance, but the training time will increase as well as memory consumption.\")));\n}\n;\nMDXContent.isMDXComponent = true;","rawBody":"Feature hashing, called the hashing trick, converts text data or categorical attributes with high cardinalities into a feature vector of arbitrary dimensionality.\n\n### Benefits\n\n-   Feature hashing is very useful for features that have high cardinality with hundreds and thousands of unique values. Hashing trick is a way to reduce the increase in dimension and memory by allowing multiple values to be present/encoded as the same value.\n\n### Feature hashing example\n\n-   First, you chose the dimensionality of your feature vectors. Then, using a hash function, you convert all values of your categorical attribute (or all tokens in your collection of documents) into a number. Then you convert this number into an index of your feature vector. The process is illustrated in the diagram below.\n\nfox3brown4quick4the0\n\nAn illustration of the hashing trick for desired dimensionality of 5 for the originality of K of values of an attributes\n\n-   Let’s illustrate what it would look like to convert the text “The quick brown fox” into a feature vector. The values for each word in the phrase are:\n    \n    ```\n    the = 5\n    quick = 4\n    brown = 4\n    fox = 3\n    ```\n    \n- Let define a hash function, hh, that takes a string as input and outputs a non-negative integer. Let the desired dimensionality be 5. By applying the hash function to each word and applying the modulo of 5 to obtain the index of the word, we get:\n    ```\n    h(the) mod 5 = 0h(quick) mod 5 = 4h(brown) mod 5 = 4h(fox) mod 5 = 3\n    ```\n\n-   In this example:\n\t- `h(the) mod 5 = 0` means that we have one word in dimension **0** of the feature vector.\n\t- `h(quick) mod 5 = 4` and `h(brown) mod 5 = 4` means that we have two words in dimension **4** of the feature vector.\n\t- `h(fox) mod 5 = 3` means that we have one word in dimension **3** of the feature vector.\n\t- As you can see, that there are no words in dimensions 1 or 2 of the vector, so we keep them as 0.\n\n-   Finally, we have the feature vector as: `[1, 0, 0, 1, 2]`.\n-   As you can see, there is a collision between words “quick” and “brown.” They are both represented by dimension 4. The lower the desired dimensionality, the higher the chances of collision. To reduce the probability of collision, we can increase the desired dimensions. This is the trade-off between speed and quality of learning.    \n\n> Commonly used hash functions are [MurmurHash3](https://en.wikipedia.org/wiki/MurmurHash), [Jenkins](https://en.wikipedia.org/wiki/Jenkins_hash_function), [CityHash](https://en.wikipedia.org/wiki/List_of_hash_functions), and [MD5](https://en.wikipedia.org/wiki/MD5).\n\n### Feature hashing in tech companies[#](https://www.educative.io/courses/machine-learning-system-design/q2AwDN4nZ73#Feature-hashing-in-tech-companies)\n\n-   Feature hashing is popular in many tech companies like [Booking](https://www.booking.com/), [Facebook](https://www.facebook.com/), [Yahoo](https://www.yahoo.com/), [Yandex](https://yandex.com/), [Avazu](http://avazu.com/home/) and [Criteo](https://www.criteo.com/).\n-   One problem with hashing is collision. If the hash size is too small, more collisions will happen and negatively affect model performance. On the other hand, the larger the hash size, the more it will consume memory.\n-   Collisions also affect model performance. With high collisions, a model won’t be able to differentiate coefficients between feature values. For example, the coefficient for “User login/ User logout” might end up the same, which makes no sense.\n\n> Depending on application, you can choose the number of bits for feature hashing that provide the correct balance between model accuracy and computing cost during model training. For example, by increasing hash size we can improve performance, but the training time will increase as well as memory consumption.","excerpt":"Feature hashing, called the hashing trick, converts text data or categorical attributes with high cardinalities into a feature vector of ar…","outboundReferences":[],"inboundReferences":[]},"tagsOutbound":{"nodes":[]}},"pageContext":{"tags":[],"slug":"/ML/Feature hashing/","sidebarItems":[{"title":"Categories","items":[{"title":"Azure","url":"","items":[{"title":"Azure Pipeline","url":"/Azure/Azure pipeline/","items":[]},{"title":"Documentation","url":"/Azure/Azure-ml/","items":[]}]},{"title":"Brain Dam","url":"/","items":[]},{"title":"Courses","url":"","items":[{"title":"Coursera","url":"","items":[{"title":"Learning How to Learn","url":"","items":[{"title":"Chunking","url":"/Courses/Coursera/Learning how to learn/Chunking/","items":[]},{"title":"Illusion of Competence","url":"/Courses/Coursera/Learning how to learn/Illusion of Competence/","items":[]},{"title":"Imposter Syndrome","url":"/Courses/Coursera/Learning how to learn/Imposter syndrome/","items":[]},{"title":"Overlearning","url":"/Courses/Coursera/Learning how to learn/Overlearning/","items":[]},{"title":"Procrastination","url":"/Courses/Coursera/Learning how to learn/Procrastination/","items":[]},{"title":"Tasks Lists","url":"/Courses/Coursera/Learning how to learn/Tasks lists/","items":[]}]}]}]},{"title":"Engineering","url":"","items":[{"title":"ACID","url":"/Engineering/Relational Databases/","items":[]},{"title":"API Design","url":"/Engineering/API Design/","items":[]},{"title":"Blob Store","url":"/Engineering/Specialized Storage Paradigms/","items":[]},{"title":"Cache for read","url":"/Engineering/Caching/","items":[]},{"title":"Client-Server Model","url":"/Engineering/Client-Server Model/","items":[]},{"title":"Configuration","url":"/Engineering/Configuration/","items":[]},{"title":"Consistent hashing with bounded load","url":"/Engineering/Consistent hashing/","items":[]},{"title":"Design Documents","url":"/Engineering/Design Documents/","items":[]},{"title":"Distributed Hash Table","url":"/Engineering/Distributed Hash Table/","items":[]},{"title":"Encryption","url":"/Engineering/Security and HTTPS/","items":[]},{"title":"Etcd","url":"/Engineering/Etcd/","items":[]},{"title":"Feature Flags","url":"/Engineering/Feature Flags/","items":[]},{"title":"Forward Proxy","url":"/Engineering/Proxies/","items":[]},{"title":"How does the load balancer work ?","url":"/Engineering/Load Balancer/","items":[]},{"title":"How to implement Leader election for your service ?","url":"/Engineering/Leader Election/","items":[]},{"title":"Idempotent Operation","url":"/Engineering/Idempotent Operation/","items":[]},{"title":"Important points","url":"/Engineering/MapReduce/","items":[]},{"title":"Latency","url":"/Engineering/Latency and Throughput/","items":[]},{"title":"Logging","url":"/Engineering/Logging and Monitoring/","items":[]},{"title":"Other strategy","url":"/Engineering/Jump hashing/","items":[]},{"title":"Other strategy","url":"/Engineering/Rendez-vous Hashing/","items":[]},{"title":"Polling","url":"/Engineering/Polling and Streaming/","items":[]},{"title":"Prometheus","url":"/Engineering/Prometheus/","items":[]},{"title":"Redis","url":"/Engineering/Redis/","items":[]},{"title":"Replication","url":"/Engineering/Replication and Sharding/","items":[]},{"title":"SLA / SLO","url":"/Engineering/Availability/","items":[]},{"title":"Socket","url":"/Engineering/Socket/","items":[]},{"title":"Storage Concept","url":"/Engineering/Storage concept/","items":[]},{"title":"Tools","url":"/Engineering/Key-Value Stores/","items":[]},{"title":"Tools","url":"/Engineering/Peer-To-Peer Networks/","items":[]},{"title":"Tools","url":"/Engineering/Publish-Subscribe Pattern/","items":[]},{"title":"Tools","url":"/Engineering/Rate Limiting/","items":[]},{"title":"Zookeeper","url":"/Engineering/Zookeeper/","items":[]}]},{"title":"Finance","url":"","items":[{"title":"Growth shares","url":"/Finance/Growth Shares/","items":[]}]},{"title":"ML","url":"","items":[{"title":"Constrained Optimization","url":"/ML/Constrained optimization/","items":[]},{"title":"Coordinate Descent","url":"/ML/Coordinate descent/","items":[]},{"title":"Determinant","url":"/ML/Determinant/","items":[]},{"title":"Dirichlet Distribution","url":"/ML/Dirichlet distribution/","items":[]},{"title":"Eigendecomposition","url":"/ML/Eigendecomposition/","items":[]},{"title":"Exploitation vs exploration","url":"/ML/Multi-armed bandit (AB testing)/","items":[]},{"title":"Feature Hashing","url":"/ML/Feature hashing/","items":[]},{"title":"Frequentist A/B testing","url":"/ML/Frequentist AB testing/","items":[]},{"title":"Gradient","url":"/ML/Gradient/","items":[]},{"title":"Gradient Descend","url":"/ML/Gradient Descend/","items":[]},{"title":"Graph Neural Network","url":"/ML/GNN/","items":[]},{"title":"Hessian","url":"/ML/Hessian/","items":[]},{"title":"Jacobian","url":"/ML/Jacobian/","items":[]},{"title":"Kernel Trick","url":"/ML/Kernel Trick/","items":[]},{"title":"KKT conditions","url":"/ML/KKT/","items":[]},{"title":"Line Search","url":"/ML/Line Search/","items":[]},{"title":"Links","url":"/ML/Latent Dirichlet Allocation/","items":[]},{"title":"Maths","url":"/ML/PCA/","items":[]},{"title":"Matrix Inverse","url":"/ML/Matrix inverse/","items":[]},{"title":"Moore-Penrose Pseudoinverse","url":"/ML/Moore-Penrose Pseudoinverse/","items":[]},{"title":"Named Entity Recognition (NER)","url":"/ML/Named Entity Recognition (NER)/","items":[]},{"title":"Newton's Method","url":"/ML/Newton's method/","items":[]},{"title":"Norms","url":"/ML/Norms/","items":[]},{"title":"Oversampling","url":"/ML/Handle Imablanced datasets/","items":[]},{"title":"Overview","url":"/ML/RANSAC/","items":[]},{"title":"Parameter Estimation","url":"/ML/Parameter estimation/","items":[]},{"title":"Positive Definite Matrix","url":"/ML/Positive Definite Matrix/","items":[]},{"title":"Regression Losses","url":"/ML/Regression losses/","items":[]},{"title":"Singular Value Decomposition (SVD)","url":"/ML/Singular Value Decomposition (SVD)/","items":[]},{"title":"Symmetric Matrix","url":"/ML/Symmetric matrix/","items":[]},{"title":"The ultimate understanding of the Naive Bayes Classifier, version 8","url":"/ML/Naive Bayes Classifier/","items":[]},{"title":"Thompson Sampling","url":"/ML/Thompson Sampling/","items":[]},{"title":"Tool","url":"/ML/Bayesian AB testing/","items":[]},{"title":"Trace of Matrix","url":"/ML/Trace of matrix/","items":[]},{"title":"Trust Region","url":"/ML/trust region/","items":[]},{"title":"Variance and Standard Error","url":"/ML/Variance and Standard error/","items":[]}]},{"title":"Papers","url":"","items":[{"title":"Questions","url":"/Papers/VICREG  variance-invariance-covariance regulaization for self-supervised Learning/","items":[]}]},{"title":"Programming","url":"","items":[{"title":"A- What is Dynamic Programming?","url":"/Programming/Dynamic Programming/","items":[]},{"title":"Example problem: Min Rewards (hard pb)","url":"/Programming/Peak and Valleys/","items":[]},{"title":"Quicksort","url":"/Programming/Quicksort/","items":[]}]},{"title":"Python","url":"","items":[{"title":"Dictionary","url":"/Python/Dictionary/","items":[]},{"title":"Is Python Compiled or Interpreted Language","url":"/Python/Is Python Compiled or Interpreted language/","items":[]}]},{"title":"Start Here","url":"/Start here/","items":[]},{"title":"Week 1","url":"/Coursera - Learning How to Learn/","items":[]}]}],"tagsGroups":[],"latestPosts":[{"fields":{"slug":"/Programming/Quicksort/","title":"Quicksort","lastUpdatedAt":"2022-05-27T09:02:01.000Z","lastUpdated":"5/27/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/Courses/Coursera/Learning how to learn/Chunking/","title":"Chunking","lastUpdatedAt":"2022-05-27T09:02:01.000Z","lastUpdated":"5/27/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/Courses/Coursera/Learning how to learn/Illusion of Competence/","title":"Illusion of Competence","lastUpdatedAt":"2022-05-27T09:02:01.000Z","lastUpdated":"5/27/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/Courses/Coursera/Learning how to learn/Imposter syndrome/","title":"Imposter Syndrome","lastUpdatedAt":"2022-05-27T09:02:01.000Z","lastUpdated":"5/27/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/Courses/Coursera/Learning how to learn/Overlearning/","title":"Overlearning","lastUpdatedAt":"2022-05-27T09:02:01.000Z","lastUpdated":"5/27/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/Courses/Coursera/Learning how to learn/Procrastination/","title":"Procrastination","lastUpdatedAt":"2022-05-27T09:02:01.000Z","lastUpdated":"5/27/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/Courses/Coursera/Learning how to learn/Tasks lists/","title":"Tasks Lists","lastUpdatedAt":"2022-05-27T09:02:01.000Z","lastUpdated":"5/27/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/Engineering/Consistent hashing/","title":"Consistent hashing with bounded load","lastUpdatedAt":"2022-05-27T09:02:01.000Z","lastUpdated":"5/27/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/Engineering/Design Documents/","title":"Design Documents","lastUpdatedAt":"2022-05-27T09:02:01.000Z","lastUpdated":"5/27/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/Engineering/Key-Value Stores/","title":"Tools","lastUpdatedAt":"2022-05-27T09:02:01.000Z","lastUpdated":"5/27/2022"},"frontmatter":{"draft":false,"tags":[]}}]}},
    "staticQueryHashes": ["2230547434","2320115945","3495835395","451533639"]}